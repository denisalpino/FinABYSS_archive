% === === === === === === === === === === === === === === === === === === === === %
%                                     Models                                      %
% === === === === === === === === === === === === === === === === === === === === %

% ModernBERT model
@article{Warner2024ModernBERT,
   abstract = {Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.},
   author = {Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
   month = {12},
   title = {Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},
   url = {http://arxiv.org/abs/2412.13663},
   year = {2024}
}

% BERT model
@inproceedings{devlin2019BERT,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers)},
  pages={4171--4186},
  year={2019}
}

% LSTM model
@article{Hochreiter1997LSTM,
   abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
   author = {Sepp Hochreiter and Jürgen Schmidhuber},
   doi = {10.1162/neco.1997.9.8.1735},
   issn = {08997667},
   issue = {8},
   journal = {Neural Computation},
   month = {11},
   pages = {1735-1780},
   pmid = {9377276},
   publisher = {MIT Press Journals},
   title = {Long Short-Term Memory},
   volume = {9},
   year = {1997}
}

% BloombrgGPT model
@article{wu2023BloombrgGpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}

% FinBERTv2 model (2020)
@article{Yang2020FinBERT,
   abstract = {Contextual pretrained language models, such as BERT (Devlin et al., 2019), have made significant breakthrough in various NLP tasks by training on large scale of unlabeled text re-sources.Financial sector also accumulates large amount of financial communication text.However, there is no pretrained finance specific language models available. In this work,we address the need by pretraining a financial domain specific BERT models, FinBERT, using a large scale of financial communication corpora. Experiments on three financial sentiment classification tasks confirm the advantage of FinBERT over generic domain BERT model. The code and pretrained models are available at https://github.com/yya518/FinBERT. We hope this will be useful for practitioners and researchers working on financial NLP tasks.},
   author = {Yi Yang and Mark Christopher Siy UY and Allen Huang},
   month = {6},
   title = {FinBERT: A Pretrained Language Model for Financial Communications},
   url = {http://arxiv.org/abs/2006.08097},
   year = {2020}
}

% FinBERTv2 model (2020)
@article{Huang2023FinBERT,
   author = {Allen Huang and Hui Wang and Yi Yang},
   doi = {10.1111/1911-3846.12832},
   issn = {19113846},
   issue = {2},
   journal = {Contemporary Accounting Research},
   month = {5},
   pages = {806-841},
   publisher = {John Wiley and Sons Inc},
   title = {FinBERT: A Large Language Model for Extracting Information from Financial Text{\textasteriskcentered}},
   volume = {40},
   year = {2023}
}

% FinBERTv1 model (2019)
@article{Araci2019FinBERT,
   abstract = {Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.},
   author = {Dogu Araci},
   month = {8},
   title = {FinBERT: Financial Sentiment Analysis with Pre-trained Language Models},
   url = {http://arxiv.org/abs/1908.10063},
   year = {2019}
}

% FinBERTv3 model (2020) the best
@techReport{Liu2020FinBERT,
   abstract = {There is growing interest in the tasks of financial text mining. Over the past few years, the progress of Natural Language Processing (NLP) based on deep learning advanced rapidly. Significant progress has been made with deep learning showing promising results on financial text mining models. However, as NLP models require large amounts of labeled training data, applying deep learning to financial text mining is often unsuccessful due to the lack of labeled training data in financial fields. To address this issue, we present FinBERT (BERT for Financial Text Mining) that is a domain specific language model pre-trained on large-scale financial corpora. In FinBERT, different from BERT, we construct six pre-training tasks covering more knowledge, simultaneously trained on general corpora and financial domain corpora, which can enable FinBERT model better to capture language knowledge and semantic information. The results show that our FinBERT outper-forms all current state-of-the-art models. Extensive experimental results demonstrate the effectiveness and robustness of FinBERT. The source code and pre-trained models of FinBERT are available on-line.},
   author = {Zhuang Liu and Degen Huang and Kaiyu Huang and Zhuang Li and Jun Zhao},
   keywords = {AI for lending: General,AI for marketing: AI for consumer sentiment analysis,AI for marketing: General,AI for payment: AI for payment risk modeling,Foundation for AI in FinTech: Analyzing big financial data,Foundation for AI in FinTech: Data mining and knowledge discovery for FinTech,Foundation for AI in FinTech: Deep learning and representation for FinTech,Foundation for AI in FinTech: General,Other areas: Financial decision-support system},
   title = {FinBERT: A Pre-trained Financial Language Representation Model for Financial Text Mining},
   url = {http://commoncrawl.org/},
   year = {2020}
}

% FinBERT + LSTM
@article{Halder2022,
   author = {Shayan Halder},
   month = {11},
   title = {FinBERT-LSTM: Deep Learning based stock price prediction using News Sentiment Analysis},
   url = {http://arxiv.org/abs/2211.07392},
   year = {2022}
}

% FinBERT + LSTM
@article{Jiang2023,
   abstract = {We apply sentiment analysis in financial context using FinBERT, and build a deep neural network model based on LSTM to predict the movement of financial market movement. We apply this model on stock news dataset, and compare its effectiveness to BERT, LSTM and classical ARIMA model. We find that sentiment is an effective factor in predicting market movement. We also propose several method to improve the model.},
   author = {Tingsong Jiang and Andy Zeng},
   month = {6},
   title = {Financial sentiment analysis using FinBERT with application in predicting stock movement},
   url = {http://arxiv.org/abs/2306.02136},
   year = {2023}
}

% FinBERT + LSTM (основная работа)
@article{Kim2023,
   author = {Jihwan Kim and Hui Sang Kim and Sun Yong Choi},
   doi = {10.3390/axioms12090835},
   issn = {20751680},
   issue = {9},
   journal = {Axioms},
   month = {9},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Forecasting the S\&P 500 Index Using Mathematical-Based Sentiment Analysis and Deep Learning Models: A FinBERT Transformer Model and LSTM},
   volume = {12},
   year = {2023}
}

% Vuković about LSTM & GBM in price forecasting
@article{Vukovi2024,
  author = {Darko B. Vuković and Sonja D. Radenković and Ivana Simeunović and Vyacheslav Zinovev and Milan Radovanović},
  doi = {10.3390/math12193066},
  issn = {22277390},
  issue = {19},
  journal = {Mathematics},
  keywords = {LSTM optimization,dynamics in market efficiency,efficient market hypothesis,forecasting,machine learning},
  month = {10},
  publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
  title = {Predictive Patterns and Market Efficiency: A Deep Learning Approach to Financial Time Series Forecasting},
  volume = {12},
  year = {2024}
}

% NNAR + sentiment scores
@article{NNAR2019,
   author = {Bruce James Vanstone and Adrian Gepp and Geoff Harris},
   doi = {10.1007/s10489-019-01458-9},
   issn = {15737497},
   issue = {11},
   journal = {Applied Intelligence},
   keywords = {Auto regressive neural networks,News,Sentiment,Stock prices,Twitter},
   month = {11},
   pages = {3815-3820},
   publisher = {Springer New York LLC},
   title = {Do news and sentiment play a role in stock price prediction?},
   volume = {49},
   year = {2019}
}

% ULMFiT for text classification
@article{howard2018ULMFIT,
  title={Universal language model fine-tuning for text classification},
  author={Howard, Jeremy and Ruder, Sebastian},
  journal={arXiv preprint arXiv:1801.06146},
  year={2018}
}

% Nomic AI Embedding models
@misc{nomic2024,
    title={Nomic Embed: Training a Reproducible Long Context Text Embedder},
    author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},
    year={2024},
    eprint={2402.01613},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

% Alibaba mGTE embedding models
@inproceedings{MGTE2024,
    title={mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval},
    author={Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Wen and Dai, Ziqi and Tang, Jialong and Lin, Huan and Yang, Baosong and Xie, Pengjun and Huang, Fei and others},
    booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track},
    pages={1393--1412},
    year={2024}
}

% Alibaba mGTE embedding models
@article{MGTE2023,
    title={Towards general text embeddings with multi-stage contrastive learning},
    author={Li, Zehan and Zhang, Xin and Zhang, Yanzhao and Long, Dingkun and Xie, Pengjun and Zhang, Meishan},
    journal={arXiv preprint arXiv:2308.03281},
    year={2023}
}

% SBERT
@inproceedings{SBERT2019,
    title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
    author = {Reimers, Nils and Gurevych, Iryna},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
    month = {11},
    year = {2019},
    publisher = {Association for Computational Linguistics},
    url = {https://arxiv.org/abs/1908.10084},
}

% HDBSCAN
@inproceedings{HDBSCAN2013,
  title={Density-based clustering based on hierarchical density estimates},
  author={Campello, Ricardo JGB and Moulavi, Davoud and Sander, J{\"o}rg},
  booktitle={Pacific-Asia conference on knowledge discovery and data mining},
  pages={160--172},
  year={2013},
  organization={Springer}
}

% cluster_selection_epsilon HDBSCAN
@inproceedings{HDBSCAN2020cluster_selection_epsilon,
   title={A Hybrid Approach To Hierarchical Density-based Cluster Selection},
   url={http://dx.doi.org/10.1109/MFI49285.2020.9235263},
   DOI={10.1109/mfi49285.2020.9235263},
   booktitle={2020 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)},
   publisher={IEEE},
   author={Malzer, Claudia and Baum, Marcus},
   year={2020},
   month=sep, pages={223–228}
 }

% Adaptive HDBSCAN
@inproceedings{HDBSCAN2022adaptive,
  title={Adaptive hierarchical density-based spatial clustering algorithm for streaming applications},
  author={Vijayan, Darveen and Aziz, Izzatdin},
  booktitle={Telecom},
  volume={4},
  number={1},
  pages={1--14},
  year={2022},
  organization={MDPI}
}

% BERTopic
@article{BERTopic2022,
  title={BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  author={Grootendorst, Maarten},
  journal={arXiv preprint arXiv:2203.05794},
  year={2022}
}

% CNN
@article{CNN1998lecun,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

% First popular CNN-LSTM in finance
@article{CNN_LSTM2020finance,
  title={A CNN-LSTM-based model to forecast stock prices},
  author={Lu, Wenjie and Li, Jiazheng and Li, Yifan and Sun, Aijun and Wang, Jingyang},
  journal={Complexity},
  volume={2020},
  number={1},
  pages={6622927},
  year={2020},
  publisher={Wiley Online Library}
}

% ParametricUMAP
@article{ParametricUMAP2020,
    author = {Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q.},
    title = {Parametric UMAP: learning embeddings with deep neural networks for representation and semi-supervised learning},
    journal = {ArXiv e-prints},
    archivePrefix = "arXiv",
    eprint = {2009.12981},
    primaryClass = "stat.ML",
    keywords = {Statistics - Machine Learning,
                Computer Science - Computational Geometry,
                Computer Science - Learning},
    year = 2020,
    }

% CPU UMAP library (AlignUMAP)
@article{mcinnes2018umap-software,
  title={UMAP: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
  journal={The Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}

% cuML (GPU ml algos)
@article{cuml2020machine,
  title={Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence},
  author={Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
  journal={arXiv preprint arXiv:2002.04803},
  year={2020}
}

% === === === === === === === === === === === === === === === === === === === === %
%                                    DATASETS                                     %
% === === === === === === === === === === === === === === === === === === === === %

% MTEB benchmark
@misc{MTEB2023,
    title={MTEB: Massive Text Embedding Benchmark},
    author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
    year={2023},
    eprint={2210.07316},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2210.07316},
}

% GLUE benchmark
@article{wang2018GLUE,
   title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
   author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
   journal={arXiv preprint arXiv:1804.07461},
   year={2018}
}

% FLUE benchmark & FLANG model
@inproceedings{FLANG2022FLUE,
   author = {Raj Sanjay Shah and Kunal Chawla and Dheeraj Eidnani and Agam Shah and Wendi Du and Sudheer Chava and Natraj Raman and Charese Smiley and Jiaao Chen and Diyi Yang},
   title = {When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain},
   booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
   year = {2022},
   publisher = {Association for Computational Linguistics}
}

% Financial PhraseBank (Sentiment Classification) dataset
@article{Malo2014FPB,
  author = {P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},
  title = {Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts},
  journal = {Journal of the Association for Information Science and Technology},
  year = {2014},
  volume = {65}
}

% FiQA 2018 Task-1 (Regression) dataset
@inproceedings{FiQA2018SA,
  author = {Maia Macedo and Siegfried Handschuh and Andr{\'e} Freitas and Brian Davis and Ross McDermott and Manel Zarrouk and Alexandra Balahur},
  title = {WWW'18 Open Challenge: Financial Opinion Mining and Question Answering},
  booktitle = {Companion Proceedings of The Web Conference 2018},
  year = {2018},
  isbn = {9781450356404},
  publisher = {International World Wide Web Conferences Steering Committee},
  address = {Republic and Canton of Geneva, CHE},
  url = {https://doi.org/10.1145/3184558.3192301},
  doi = {10.1145/3184558.3192301},
  abstract = {The growing maturity of Natural Language Processing (NLP) techniques and resources is dramatically changing the landscape of many application domains that depend on the analysis of large-scale unstructured data. The finance domain, with its reliance on interpreting multiple unstructured and structured data sources and its demand for fast, comprehensive decision making, has already emerged as a primary ground for experimenting with NLP, Web Mining, and Information Retrieval (IR) techniques for the automatic analysis of financial news and opinions online. This challenge focuses on advancing the state-of-the-art of aspect-based sentiment analysis and opinion-based Question Answering for the financial domain.},
  pages = {1941--1942},
  numpages = {2},
  keywords = {financial domain, opinion mining, question answering},
  location = {Lyon, France},
  series = {WWW '18}
}

% NHC dataset
@inproceedings{sinha2021NHC,
  title={Impact of news on the commodity market: Dataset and results},
  author={Sinha, Ankur and Khandait, Tanmay},
  booktitle={Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2},
  pages={589--601},
  year={2021},
  organization={Springer}
}

% FinNER dataset
@inproceedings{alvarado2015FinNER,
  title={Domain adaption of named entity recognition to support credit risk assessment},
  author={Alvarado, Julio Cesar Salinas and Verspoor, Karin and Baldwin, Timothy},
  booktitle={Proceedings of the australasian language technology association workshop 2015},
  pages={84--90},
  year={2015}
}

% FinSBD-3
@inproceedings{Au2021FinSBD,
   author = {Au, Willy and Ait-Azzi, Abderrahim and Kang, Juyeon},
   title = {FinSBD-2021: The 3rd Shared Task on Structure Boundary Detection in Unstructured Text in the Financial Domain},
   year = {2021},
   isbn = {9781450383134},
   publisher = {Association for Computing Machinery},
   address = {New York, NY, USA},
   url = {https://doi.org/10.1145/3442442.3451378},
   doi = {10.1145/3442442.3451378},
   abstract = {Document processing is a foundational pre-processing task in natural language application applied in the financial domain. In this paper, we present the result of FinSBD-3, the 3rd shared task on Structure Boundary Detection in unstructured text in the financial domain. The shared task is organized as part of the 1st Workshop on Financial Technology on the Web. Participants were asked to create system detecting the boundaries of elements in unstructured text extracted from financial PDF. This edition extends the previous shared tasks by adding boundaries of visual elements such as tables, figures, page headers and page footers; on top of sentences, lists and list items which were already present in previous edition of the shared tasks.},
   booktitle = {Companion Proceedings of the Web Conference 2021},
   pages = {276–279},
   numpages = {4},
   keywords = {computer vision, document parsing, document segmentation, natural language processing, sentence segmentation, shared task},
   location = {Ljubljana, Slovenia},
   series = {WWW '21}
}

% RULER dataset
@article{hsieh2024RULER,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}

% Needle In A Haystack dataset
@misc{Kamradt2023,
  title={Needle In A Haystack - Pressure Testing LLMs},
  author={Gregory Kamradt},
  publisher={Github},
  year={2023}
}

% CoLA dataset
@inproceedings{dudy2018CoLA,
    title = {A Multi-Context Character Prediction Model for a Brain-Computer Interface},
    author = {Dudy, Shiran  and
      Xu, Shaobin  and
      Bedrick, Steven  and
      Smith, David},
    editor = {Faruqui, Manaal  and
      Sch{\"u}tze, Hinrich  and
      Trancoso, Isabel  and
      Tsvetkov, Yulia  and
      Yaghoobzadeh, Yadollah},
    booktitle = {Proceedings of the Second Workshop on Subword/Character {LE}vel Models},
    month = {6},
    year = {2018},
    address = {New Orleans},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/W18-1210/},
    doi = {10.18653/v1/W18-1210},
    pages = {72--77},
    abstract = {Brain-computer interfaces and other augmentative and alternative communication devices introduce language-modeing challenges distinct from other character-entry methods. In particular, the acquired signal of the EEG (electroencephalogram) signal is noisier, which, in turn, makes the user intent harder to decipher. In order to adapt to this condition, we propose to maintain ambiguous history for every time step, and to employ, apart from the character language model, word information to produce a more robust prediction system. We present preliminary results that compare this proposed Online-Context Language Model (OCLM) to current algorithms that are used in this type of setting. Evaluation on both perplexity and predictive accuracy demonstrates promising results when dealing with ambiguous histories in order to provide to the front end a distribution of the next character the user might type.}
}

% SST-2 dataset
@inproceedings{socher2013SST2,
    title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
    author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
    booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
    month = {10},
    year = {2013},
    address = {Seattle, Washington, USA},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D13-1170},
    pages = {1631--1642},
}

% MRPC dataset
@inproceedings{dolan2005MRPC,
    title = {Automatically Constructing a Corpus of Sentential Paraphrases},
    author = {Dolan, William B. and Brockett, Chris},
    booktitle = {Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)},
    year = {2005},
    url = {https://aclanthology.org/I05-5002/}
}

% STS-B dataset
@inproceedings{Cer2017STSB,
   title={SemEval-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation},
   url={http://dx.doi.org/10.18653/v1/S17-2001},
   DOI={10.18653/v1/s17-2001},
   booktitle={Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
   publisher={Association for Computational Linguistics},
   author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
   year={2017}
}

% QQP dataset
@misc{chen2018QQP,
   title={Quora question pairs.},
   author={Z. Chen and H. Zhang and X. Zhang and L. Zhao},
   year={2017}
}

% MNLI dataset
@misc{williams2018MNLI,
      title={A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
      author={Adina Williams and Nikita Nangia and Samuel R. Bowman},
      year={2018},
      eprint={1704.05426},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1704.05426},
}

% QNLI dataset
@article{rajpurkar2016QNLI,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

% RTE dataset
@article{bentivogli2009RTE,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  journal={TAC},
  volume={7},
  number={8},
  pages={1},
  year={2009},
  publisher={Citeseer}
}

% WNLI dataset
@article{levesque2012WNLI,
  title={The Winograd schema challenge.},
  author={Levesque, Hector J and Davis, Ernest and Morgenstern, Leora},
  journal={KR},
  volume={2012},
  pages={13th},
  year={2012}
}

% === === === === === === === === === === === === === === === === === === === === %
%                                  ALGORITHMS                                     %
% === === === === === === === === === === === === === === === === === === === === %

% Domain-Adaptive Pretraining algorithm
@article{gururangan2020DAPT,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}

% Attention algotithm
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% Transformers for Time Series
@article{wen2022transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  journal={arXiv preprint arXiv:2202.07125},
  year={2022}
}

% Early/Slow/Late Fusion overview
@inproceedings{Karpathy_2014_CVPR,
    author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
    title = {Large-scale Video Classification with Convolutional Neural Networks},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {6},
    year = {2014}
}

% Early Fusion
@article{dutt2022shared,
  title={Shared manifold learning using a triplet network for multiple sensor translation and fusion with missing data},
  author={Dutt, Aditya and Zare, Alina and Gader, Paul},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  volume={15},
  pages={9439--9456},
  year={2022},
  publisher={IEEE}
}

% Late Fusion
@article{ortega2019multimodal,
  title={Multimodal fusion with deep neural networks for audio-video emotion recognition},
  author={Ortega, Juan DS and Senoussaoui, Mohammed and Granger, Eric and Pedersoli, Marco and Cardinal, Patrick and Koerich, Alessandro L},
  journal={arXiv preprint arXiv:1907.03196},
  year={2019}
}

% Late Fusion
@inproceedings{joze2020mmtm,
  title={MMTM: Multimodal transfer module for CNN fusion},
  author={Joze, Hamid Reza Vaezi and Shaban, Amirreza and Iuzzolino, Michael L and Koishida, Kazuhito},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13289--13299},
  year={2020}
}

% Slow Fusion
@inproceedings{feichtenhofer2016convolutional,
  title={Convolutional two-stream network fusion for video action recognition},
  author={Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1933--1941},
  year={2016}
}

% Flash Attention
@misc{flash2022attention,
    title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
    author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
    year={2022},
    eprint={2205.14135},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2205.14135},
}

% Contrastive learning STE (SimCSE)
@article{gao2021simcse,
  title={Simcse: Simple contrastive learning of sentence embeddings},
  author={Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  journal={arXiv preprint arXiv:2104.08821},
  year={2021}
}

% Mixture-of-Experts initial
@article{jacobs1991adaptive,
		author = {Jacobs, Robert and Jordan, Michael and Nowlan, Steven and Hinton, Geoffrey},
		year = {1991},
		month = {03},
		pages = {79-87},
		title = {Adaptive Mixtures of Local Experts},
		volume = {3},
		journal = {Neural Computation},
		doi = {10.1162/neco.1991.3.1.79}
}

% Mixture-of-Experts effectivety and economy
@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

% Mixture-of-Experts Google trillion parameters
@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

% TPE and BO overview
@article{TPEandBO2011bergstra,
  title={Algorithms for hyper-parameter optimization},
  author={Bergstra, James and Bardenet, R{\'e}mi and Bengio, Yoshua and K{\'e}gl, Bal{\'a}zs},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

% TPE overview
@article{TPE2023watanabe,
  title={Tree-structured parzen estimator: Understanding its algorithm components and their roles for better empirical performance},
  author={Watanabe, Shuhei},
  journal={arXiv preprint arXiv:2304.11127},
  year={2023}
}

% HPO overview
@article{HPOoverview2015shahriari,
  title={Taking the human out of the loop: A review of Bayesian optimization},
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and De Freitas, Nando},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={148--175},
  year={2015},
  publisher={IEEE}
}

% Hyperband
@article{hyperband2018li,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={185},
  pages={1--52},
  year={2018}
}

% BOHB
@inproceedings{BOHB2018falkner,
  title={BOHB: Robust and efficient hyperparameter optimization at scale},
  author={Falkner, Stefan and Klein, Aaron and Hutter, Frank},
  booktitle={International conference on machine learning},
  pages={1437--1446},
  year={2018},
  organization={PMLR}
}

% Library CMAES
@article{CMAES2024nomura,
  title={cmaes: A simple yet practical python library for cma-es},
  author={Nomura, Masahiro and Shibata, Masashi},
  journal={arXiv preprint arXiv:2402.01373},
  year={2024}
}

% CMAES with learning rate
@inproceedings{lrCMAES2023nomura,
  title={CMA-ES with learning rate adaptation: Can CMA-ES with default population size solve multimodal and noisy problems?},
  author={Nomura, Masahiro and Akimoto, Youhei and Ono, Isao},
  booktitle={Proceedings of the Genetic and Evolutionary Computation Conference},
  pages={839--847},
  year={2023}
}

% CMAES with warm starting
@inproceedings{warmCMAES2021nomura,
  title={Warm starting CMA-ES for hyperparameter optimization},
  author={Nomura, Masahiro and Watanabe, Shuhei and Akimoto, Youhei and Ozaki, Yoshihiko and Onishi, Masaki},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={35},
  number={10},
  pages={9188--9196},
  year={2021}
}

% CMAES with IPOP restarting strategy
@inproceedings{restartCMAES2005auger,
  title={A restart CMA evolution strategy with increasing population size},
  author={Auger, Anne and Hansen, Nikolaus},
  booktitle={2005 IEEE congress on evolutionary computation},
  volume={2},
  pages={1769--1776},
  year={2005},
  organization={IEEE}
}

% === === === === === === === === === === === === === === === === === === === === %
%                                     OTHERS                                      %
% === === === === === === === === === === === === === === === === === === === === %

% Efficiency Market Theory
@article{emt1970fama,
		ISSN = {00221082, 15406261},
		URL = {http://www.jstor.org/stable/2325486},
		author = {Eugene F. Fama},
		journal = {The Journal of Finance},
		number = {2},
		pages = {383--417},
		publisher = {[American Finance Association, Wiley]},
		title = {Efficient Capital Markets: A Review of Theory and Empirical Work},
		urldate = {2025-04-30},
		volume = {25},
		year = {1970}
}

% Sentiment Analysis Taxonomy
@Inbook{SA2020taxonomy,
		author="Pathak, Ajeet Ram
						and Agarwal, Basant
						and Pandey, Manjusha
						and Rautaray, Siddharth",
		editor="Agarwal, Basant
						and Nayak, Richi
						and Mittal, Namita
						and Patnaik, Srikanta",
		title="Application of Deep Learning Approaches for Sentiment Analysis",
		bookTitle="Deep Learning-Based Approaches for Sentiment Analysis",
		year="2020",
		publisher="Springer Singapore",
		address="Singapore",
		pages="1--31",
		isbn="978-981-15-1216-2",
		doi="10.1007/978-981-15-1216-2_1",
		url="https://doi.org/10.1007/978-981-15-1216-2_1"
}

% Financial Sentiment Analysis Taxonomy: Techniques and Applications
@article{FSA2024techniques,
   author = {Kelvin Du and Frank Xing and Rui Mao and Erik Cambria},
   doi = {10.1145/3649451},
   issn = {15577341},
   issue = {9},
   journal = {ACM Computing Surveys},
   keywords = {Financial sentiment analysis,deep learning,financial forecasting,information system,machine learning,natural language processing},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {Financial Sentiment Analysis: Techniques and Applications},
   volume = {56},
   year = {2024}
}

% Financial Sentiment Analysis problems
@inproceedings{FSA2020problems,
  title={Financial sentiment analysis: An investigation into common mistakes and silver bullets},
  author={Xing, Frank and Malandri, Lorenzo and Zhang, Yue and Cambria, Erik},
  booktitle={Proceedings of the 28th international conference on computational linguistics},
  pages={978--987},
  year={2020}
}

% Cross-consensus method
@book{consensus1997bogdan,
  title={Qualitative research for education},
  author={Bogdan, Robert and Biklen, Sari Knopp},
  volume={368},
  year={1997},
  publisher={Allyn \& Bacon Boston, MA}
}

% Comparison of clustering algorithms
@article{chen2024fast,
  title={Fast and explainable clustering based on sorting},
  author={Chen, Xinye and G{\"u}ttel, Stefan},
  journal={Pattern Recognition},
  volume={150},
  pages={110298},
  year={2024},
  publisher={Elsevier}
}

% Optuna
@inproceedings{optuna2019akiba,
  title={{O}ptuna: A Next-Generation Hyperparameter Optimization Framework},
  author={Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  booktitle={The 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2623--2631},
  year={2019}
}

% Ray Tune
@article{raytune2018liaw,
    title={Tune: A Research Platform for Distributed Model Selection and Training},
    author={Liaw, Richard and Liang, Eric and Nishihara, Robert
            and Moritz, Philipp and Gonzalez, Joseph E and Stoica, Ion},
    journal={arXiv preprint arXiv:1807.05118},
    year={2018}
}