This paper addresses one of the most promising and at the same time difficult tasks of modern
financial analytics - interpreted asset value forecasting from the point of view
of efficient market theory based on aspect-oriented news flow analysis using
modern deep learning architectures. The rapid development of LLMs, as well as their application in
financial analytics tasks, have demonstrated significant progress compared to classical approaches.,
However, at the same time, a number of unresolved problems were identified, both regarding the architectural limitations
of the models and the lack of specialized data and means of interpreting the results. In the framework
of this study, an attempt was made to integrate several modern paradigms (DAPT,
ABSA, DTM, hybrid clustering, etc.) into a single analytical solution, taking into account the specifics of the financial
domain and the transparency requirements of the model.

The key result of the work was the design and implementation of an application architecture that allows
process long financial texts (up to 8,192 tokens), taking into account their semantic and aspect structure.
For this purpose, the latest ModernBERT architecture was used, which has a number of significant advantages
over previous versions of FinBERT and BERT. Among them are an increased contextual window, rotational
positional encodings, an alternating mechanism of attention and optimization, focused on working with large
documents. This made it possible to process not only headlines or short excerpts, as in most
previous research, but also complete articles, press releases, analytical reviews, as well as transform
the semantics of long texts into vector representations suitable for clustering and subsequent analysis.

One of the most significant contributions of this work is the construction of a pipeline of thematic modeling,
in which aspects are considered as thematic clusters allocated in the embedding space of texts
using methods of dimensionality reduction (UMAP) and hierarchical clustering based on density.
(HDBSCAN). This approach avoids the need for manual definition of aspects and pre-fixed
dictionaries, which is especially important in the context of the constantly changing terminology, vocabulary and agenda of the financial
domain. Unlike the classic ABSA, in the proposed solution, aspects are extracted at the level
of multiple patent themes, which makes the system adaptive and able to scale to new information flows
without manual markup.

The developed system combines complementary components: a deep embedding model,
a component of thematic aggregation, clustering of texts, density validation metrics, and
an interactive semantic map visualizer. The combined use of these components ensures not only high
efficiency, but also interpretability, which is an essential requirement in management and financial
analytics. For example, the proposed mechanism for generating topic names using GPT-4o and ranking
terms based on c-TF-IDF and MMR allows you to visually imagine which keywords are used to generate
this or that topic, as well as which documents make the main contribution to it.

The study identified and analyzed several significant limitations that arise
in the process of building such a system. In particular, the lack of open specialized
financial text corpora led to the need to create its own news corpus with Yahoo! Finance,
which has exceeded 15 GB in size. A number of technical and theoretical problems were also identified: lack
of consensus on the methodology of semantic deduplication, difficulties with processing long documents (for example,
10-K reports), as well as the inability to fairly compare models using existing benchmarks
based on short texts (for example, FLUE). To overcome these problems, the author
's solutions were proposed, including a mathematical formalization of deduplication based on semantic volume.

The metrics for assessing the quality of clustering and thematic modeling were also scrupulously considered in this work
. In addition to the standard silhouette coefficient, the DBCV index was used --- specialized
a metric for estimating clusters of arbitrary shape, taking into account density. The results showed that the application
DBCV allows you to correctly reflect the quality of clustering in problems where classical metrics (for example,
the Davis–Bolden index, Kalinski–Kharabash or VIASCKDE) give a distorted picture.

Special attention in the work was paid not only to architectural, but also to methodological rigor.
The pipeline construction stages were thought out taking into account the requirements of reproducibility and scalability, as well as
potential integration with quantitative models. Since the task of aspect analysis is not only
a tool for understanding texts, but also a means of improving the forecast of asset value dynamics, the proposed
solution is aimed at subsequent integration with quantitative predictors. Thus, the work lays
the foundation for hybrid forecasting approaches that combine qualitative and quantitative sources
of information. This research is a kind of bridge between the latest discoveries in the field of deep
learning and the field of finance.

An important theoretical contribution is also the discussion of the relationship between thematic modeling and aspect
analysis. This paper shows that aspects in financial texts can be interpreted as latent
topics identified by clustering in the embedding space. This paves the way for the unification of two
previously disparate areas of text analysis — thematic modeling and aspect analysis of sentiment.
This approach is especially effective in the absence of marked-up data and highly variable topics.
financial publications. On the other hand, the study offers a new approach that allows us to look
at the definition and the very idea of the tonality of texts from a different, unbiased angle.

Summing up, we can formulate the main achievements of the work.:

\begin{enumerate}
    \item The architecture of the interpreted system of aspect-oriented analysis of financial news is implemented
    based on ModernBERT, UMAP and HDBSCAN.
    \item Implemented an approach to the automatic extraction of topics (aspects) without the use of dictionaries and manual markup,
    to enable subsequent aggregation of sentiment at the topic level.
    \item A detailed preprocessing and collection of a specialized financial news corpus has been carried out, taking into account
the specifics of the texts (length, duplication, sources). The case is estimated at 1 billion tokens and is of high quality
    It is designed to include rich metadata, allowing it to be used in related research.
    A mathematical approach to semantic deduplication has been developed and formalized, taking into account the limitations of
    textual comparison and the need for semantic uniqueness analysis.
    \item The architectural advantages of ModernBERT compared to BERT and FinBERT are analyzed,
    The directions for further improving the interpretability of models have also been identified.
    \item Has developed a highly functional system for analytics in the field of semantics of financial publications,
    which functions in real time.
\end{enumerate}

Despite the results achieved, the work leaves open a number of areas for future research.
First, it is necessary to train a designed architecture, including experts and a predictive model,
with the integration of time series and quantitative features to build a full-fledged hybrid predictor.
Secondly, the development of a multi-agent architecture is promising, where models of various levels (for example,
specialized for different types of documents) interact within a single pipeline, which is
an open field for the development of a financial assistant for the semantic analysis of publications. Third,,
Further study of dynamic thematic modeling methods may allow for more accurate
and interpretable tracking of the evolution of aspects over time, which is critically important in a volatile
information field.

In general, the presented work lays the foundation for a systematic approach to financial forecasting
using large language models in the context of effective market theory. The combination of engineering
implementation, mathematical analysis and theoretical justification makes the proposed solution not only
practice-oriented, but also significant in terms of scientific novelty.