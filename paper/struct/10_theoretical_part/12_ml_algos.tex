\subsubsection{Dimensionality Reduction}
\textbf{Principal Component Analysis (PCA)} is a linear dimensionality reduction method based
on the decomposition of the covariance matrix of the original data. Given a zero trait mean,
it finds the covariance eigenvectors (components) responsible for the maximum variance:
$\Sigma = X^T X$, $\Sigma w_i = \lambda_i w_i$, the projection of the data $Y = XW_{d'}$
onto the first $d'$ eigenvectors.

PCA aims to preserve as much variance of the original data as possible and, as a consequence,
preserves well the “global” cluster structure \parencite{TRIMAP2019}. In practice, PCA is often
used as an intermediate step: for example, when dealing with high-dimensional text embeddings
(several hundred features, e.g., embeddings of dimensionality 768) before non-linear
dimensionality reduction methods. The use of PCA can significantly reduce the dimensionality
(down to tens to hundreds of components) and speed up subsequent computations\parencite{huang2022towards}.

However, PCA is a linear method, and it does not capture the more complex nonlinear dependencies
inherent in language embeddings. As a consequence, subtle local word-document relationships
may be lost, although the overall structure (the “global landscape” of the data) is most often
preserved.

\textbf{t-distribution Stochastic Meighbor Embedding (t-SNE)} is a nonlinear stochastic method focused
on preserving local data structure. In high-dimensional space, it computes conditional probabilities
$p_{j|i} \propto \exp(-|x_i-x_j|^2/2\sigma_i^2)$, reflecting the proximity of neighbors; it then
symmetrizes them: $p_{ij}=(p_{j|i}+p_{i|j})/2n$. A similar measure (Student's distribution with 1 degree
of freedom) is given in the low-dimensional mapping. The algorithm optimizes the placement of $Y$ points
by minimizing the Kullback-Leibler divergence $C = \sum_{i,j} p_{ij}\ln\frac{p_{ij}}{q_{ij}}$.
As a result, points close to each other in the original space will retain a local cluster structure
in the low-dimensional space.

The 'perplexity' hyperparameter determines the number of effective neighbors. t-SNE shows an impressive
visualization of local clusters, but poorly reproduces the global distance between clusters.
It is computationally expensive for large samples and is usually used only for the final transition
to 2D space (or 3D), not for intermediate dimensionality reduction. For textual embeddings, t-SNE is
often applied after preprocessing (e.g., PCA dimensionality reduction) because it scales poorly directly
to several hundred features.

\textbf{Uniform Manifold Approximation and Projection (UMAP)} is a method of nonlinear dimensionality reduction
based on the manifold assumption. UMAP theoretically relies on Riemannian geometry and the theory
of “fuzzy simplicial sets”. The algorithm constructs a graph of $k$-nearest neighbors
in the original space, then each pair of points is assigned a “membership” in a fuzzy set by a formula of the form:

\begin{equation}
    \mu_{ij}=\exp\big(-\cfrac{\max(0, d(x_i, x_j) - \rho_i)}{\sigma_i}\big),
\end{equation}

where $\rho_i$ takes into account the density of neighbors. These local symplectic sets are then combined
and symmetrized, and a weighted data graph is obtained. Next, a similar “fuzzy” graph is constructed in
low-dimensional space and the location of points is optimized by minimizing the cross entropy energy between
the two graphs.

UMAP retains the local structure of the data while aiming to distribute points uniformly on the manifold;
unlike t-SNE, it can better retain some global features (due to the cross-entropy used). The main hyperparameters
of UMAP are the number of neighbors 'n\_neighbors' (specifies the scale of locality) and 'min\_dist'
(minimum distance of points in the mapping).

UMAP demonstrates high speed and scalability (the method can run on an arbitrary number of output measurements
at once). In an experiment, UMAP was shown to give a visualization quality comparable or better than t-SNE
with a significantly shorter runtime \parencite{UMAP2018mcinnes}. UMAP's advantages also include the preservation
of a larger-scale data structure and the ability to “downscale” the dimensionality down to multidimensional
vectors (not only 2D).

Nevertheless, UMAP may incorrectly represent highly sparse clusters by “equalizing” dense and sparse regions
(the algorithm actually seeks a uniform distribution of data on the assumed manifold). In addition, UMAP
results are sensitive to the choice of hyperparameters and the degree of noise sampling (the algorithm uses
approximate neighbor search and negative sample sampling) \parencite{huang2022towards}. However, in most textual
data clustering applications, UMAP has proven to be a robust and efficient tool.

There are several Python implementations of this method. The classical implementation, 'umap-learn' (CPU),
is widely used \parencite{mcinnes2018umap-software}, and there is a GPU implementation ('cuML') to speed
up the learning process \parencite{cuml2020machine}. cuML's GPU implementation of UMAP can give speedups
of up to 10-100× compared to the CPU version on large amounts of data. However, in early versions of cuML,
approximations were introduced for speed purposes, sometimes resulting in a small difference in display
quality compared to the original. Also the GPU implementation of UMAP may reduce the local structure preservation
relative to the reference version. Thus, 'cuML' UMAP is suitable for very large amounts of data, while
the implementation from 'umap-learn' is more versatile and stable, but runs slower.

\textbf{Triplet Manifold Approximation and Projection (TriMAP)} is an embedding learning method with an emphasis
on the global \parencite{TRIMAP2019} data structure. It formulates the problem through triples of $(i,j,k)$
points: point $i$ must be closer to $j$ than to $k$ in a low-dimensional representation. The selection
of such triplets is based on nearest and farthest neighbors in the original space; each triplet is given
a weight reflecting the relative closeness of the pairs in the original space. Optimization is performed over
a large sample of informative triplets using gradient descent.

TriMAP preserves global structure much better than t-SNE and often better than UMAP. TriMAP also scales well
and exhibits low runtime for large and high-dimensional samples \parencite{TRIMAP2019}. In terms of text embeddings,
TriMAP can provide a more readable picture of document cluster locations on 2D (although local community details
may be smoothed out).

\textbf{Pairwise Controlled Manifold Approximation and Projection (PaCMAP)} is a newer method specifically designed
to balance local and global structure \parencite{PACMAP2021}. Like TriMAP, it uses samples of pairs of points
of different types: “close” pairs (neighbors), "middle" pairs (between clusters), and "far" pairs. For each
pair type, the corresponding weights and attraction/repulsion forces are specified. As a result, the optimized
loss function tends to simultaneously compress locally close points and push distant ones apart, preserving
the global shape of the distribution.

PaCMAP is robust to hyperparameter selection and dimensionality reduction in preprocessing, and preserves both
local and global structure well. The disadvantages of PaCMAP are its relative novelty and the need to fit
fractions of different types of pairs.

Systematic comparisons indicate a characteristic separation in the properties of these algorithms. Thus, PCA,
TriMAP, and PaCMAP preserve global distances (large-scale cluster structure) well, while t-SNE and UMAP are better
at capturing local details \parencite{huang2022towards}. PCA is traditionally used for preprocessing: reducing
the dimensionality to tens of components speeds up further analysis and makes it more stable. However, it is
noticeable that full PCA preprocessing can distort the original distances, so the results of the final embedding
(e.g., t-SNE/UMAP visualization) often depend on the number of PCA components. In experimental method evaluations,
PaCMAP and TriMAP showed the best agreement of global distances, while UMAP and t-SNE were on average inferior
in this task. Conversely, t-SNE and UMAP performed best in the classification task on vector features (testing
local consistency).

In general, the choice of dimensionality reduction method for high-dimensional embeddings of documents depends
on the task: for subsequent clustering and thematic one often uses PCA or UMAP (for more “stable” representation
of clusters), and for final 2D visualization and detailed analysis of local clusters --- t-SNE, UMAP or PaCMAP.
Careful selection of hyperparameters and possibly a combination of methods (e.g., PCA+UMAP) can achieve a better
representation of the textual data structure.

\subsubsection{Clustrering}
In the considered scheme for thematic analysis of financial news articles, a dimensionality
reduction algorithm is applied after obtaining textual embeddings. After this dimensionality
reduction, clustering algorithms operate in a less sparse space, which can improve
the selection of dense regions and reduce the influence of noise factors.

\textbf{K-Means} is one of the classical partitional clustering algorithms \parencite{kmeans2010data}.
It seeks to partition the data into $K$ clusters by minimizing the intra-cluster point spread.
We denote clusters $C_1,\dots,C_K$ and cluster centroids $\mu_k$; then the optimized objective
function (the “sum of squares of distances” metric from points to the corresponding centroids)
is given as

\begin{equation}
    J(C)= \sum_{k=1}^K \sum_{x_i \in C_k} \|x_i - \mu_k\|^2,
\end{equation}

and is minimized when partitioned by the Euclidean metric. Finding the global minimum of this
function is an NP-complete problem, so the K-Means method performs a greedy iterative procedure
of point reclassification and centroid recalculation (usually with random initialization) that
converges to a local minimum. An important feature of K-Means is the requirement to specify
the number of clusters $K$ and the initial approximations in advance.

Since the algorithm typically uses a Euclidean metric, it forms mostly spherical clusters. All
objects are automatically assigned to some cluster (rigidly “belonging” each point to one cluster),
and the algorithm does not explicitly emphasize outliers or noise. Advantages of the method include
ease of implementation, low computational cost, and widespread use. However, K-Means is unstable
to outliers, and is poor at distinguishing nested or highly heterogeneous clusters.

\textbf{Density-Based Spatial Clustering of Applications with Noise (DBSCAN)} is a density-based
clustering method \parencite{DBSCAN1996}. It defines clusters as regions of high-density data separated
by low-density regions. The algorithm uses two parameters: the radius $\epsilon$ and the minimum
number of points $min_{pts}$. A point is called the “core” of a cluster if its $\epsilon$-neighborhood
contains at least $min_{pts}$ points. Points reachable in density from the core belong to the same
cluster.

DBSCAN automatically separates points that are not in dense regions as noise and does not require
the number of clusters to be specified. Due to this, the method finds clusters of arbitrary shape
and is well suited for datasets with non-uniformly distributed objects. However, DBSCAN has significant
limitations: the choice of a single $\epsilon$ threshold is critical, and when combining clusters
of different densities, the algorithm either merges them into one whole or breaks them into too small
fragments. In addition, as the dimensionality of the space grows, the data becomes sparse, and it becomes
difficult to distinguish a high-density region from a low-density one. As a consequence, DBSCAN
in high-dimensional text embeddings often demonstrates reduced efficiency. Also, the complexity
of classical DBSCAN in the absence of optimization can reach $O(n^2)$, although practical implementations
with indexes usually have significantly lower performance.

\textbf{Hierarchical DBSCAN (HDBSCAN)} is a hierarchical extension of the DBSCAN method \parencite{HDBSCAN2013}.
Unlike DBSCAN, HDBSCAN does not require a fixed $\epsilon$: instead, it computes the mutual reachability
distance between points $x$ and $y$ as

\begin{equation}
    d_{\mathrm{mreach}}(x,y) = \max\bigl\{d_{\mathrm{core}}(x),\,d_{\mathrm{core}}(y),\,d(x,y)\bigr\},
\end{equation}

where $d_{\mathrm{core}}(x)$ is the distance from a point $x$ to its $k$-th nearest neighbor (i.e.,
the minimum $\epsilon$ at which $x$ becomes the “core” of the cluster). We then construct a graph
of complete mutual reachability (or directly a minimal island tree at such distances). By removing
the edges of this tree in descending order of weight, the algorithm generates a tree-like hierarchy
of clusters reflecting the nested structure of the data at different density thresholds. From this tree,
the final clusters are selected based on the stability criterion. Such a procedure is equivalent
to performing multiple runs of DBSCAN at all possible $\epsilon$ and selecting the most significant
“stable” clusters. An important feature of HDBSCAN is that it finds the optimal number of clusters
by itself, requiring only a minimum cluster size ('min\_cluster\_size').

Thanks to its hierarchical approach, HDBSCAN is able to detect nested clusters of different densities
and adapt more flexibly to the data distribution than DBSCAN and K-Means. The algorithm is robust
to density fluctuations: if there are regions of different homogeneity in the data, HDBSCAN will identify
large sparse clusters and smaller dense clusters simultaneously. It automatically flags outliers, similar
to DBSCAN, but without rigidly binding to a single threshold. Because of the additional processing
(searching for $k$-nearest neighbors, MST construction, and hierarchy analysis), HDBSCAN is somewhat
more computationally complex, but modern implementations with efficient neighborhood structures usually
provide comparable or even better performance \parencite{HDBSCAN2017software}. In general, HDBSCAN
provides a richer description of the data structure at different levels of granularity and more often
yields more meaningful clusters in complex multidimensional spaces.

Existing experimental studies show that in the task of topic analysis of long texts, each method has its
own pros and cons. K-Means is often used as a basic method due to its simplicity and scalability, but
it gives a relatively coarse partitioning of topics because it is limited by the spherical shape
of clusters and requires the number of topics to be specified in advance. DBSCAN can detect arbitrarily
shaped clusters and separate noise, but its performance is reduced on high-dimensional embeddings of texts
due to sparsity and the need to tune the global density threshold.

In many comparative experiments, HDBSCAN is shown to outperform both mentioned methods in terms
of the quality of topic clusters: it automatically adapts to the variability of embedding density, detects
nested topics of different granularity, and reliably eliminates irrelevant noise
\parencite{HDBSCAN2017software, HDBSCAN2013}. These findings are supported by practical applications
(e.g., news article clustering), where HDBSCAN most often yields more interpretable and stable results
compared to K-Means or “flat” DBSCAN \parencite{BERTopic2022}.

\subsubsection{Evaluation}
Silhouette Index + Stability Index + DBCV