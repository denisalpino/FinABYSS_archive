\subsubsection{Price Prediction}
There exists a variety of approaches demonstrating the effectiveness of asset price prediction using deep
neural networks. Recurrent neural networks (RNN) are undeniably the most prevalent for this task, while
convolutional neural networks (CNN) are often employed as auxiliary components.

The long short-term memory (LSTM) \parencite{Hochreiter1997LSTM} architecture is the most representative
member of the RNN family and is frequently applied to price forecasting. Its adaptations ---
such as bidirectional LSTM (Bi-LSTM) and hybrid LSTM+CNN models --- also exhibit high performance.
Furthermore, with the advent of the Transformer \parencite{vaswani2017attention} architecture,
research has increasingly focused on adapting it to the characteristics of time series
\parencite{wen2022transformers}.

Among recent experiments, the following are particularly noteworthy:
\begin{itemize}
    \item A repository demonstrating the potential of the Transformer architecture for Bitcoin price
    forecasting\footnote{URL: \url{https://github.com/baruch1192/-Bitcoin-Price-Prediction-Using-Transformers}};
    \item A study comparing the performance of Bi-LSTM, hybrid Bi-LSTM+CNN, and Transformer models
    in predicting IBM stock prices\footnote{URL: \url{https://github.com/JanSchm/CapMarket}};
    \item An LSTM-based trading bot designed to capture short-term profits during sideways price
    movements of the CGEN asset, which achieved a 4\% daily return on deposit in backtesting\footnote{URL: \url{https://github.com/roeeben/Stock-Price-Prediction-With-a-Bot}}.
\end{itemize}

Nevertheless, individual models --- whether LSTM-based or decision tree--based --- have limited
adaptability to changing market regimes and struggle to adjust to their dynamics \parencite{Vukovi2024}.
Although this limitation is evident, it is often underestimated in the literature. According
to the Efficient Market Theory (EMT) \parencite{emt1970fama}, asset prices incorporate all available
market information, which casts doubt on the feasibility of precise forecasting using only
historical quantitative indicators (OHLC data, trading volume, and classical technical indicators).

Moreover, most models fail to account for nuanced factors such as limit order book dynamics,
interactions with other trading algorithms, and qualitative off-exchange information. Recent
studies indicate that integrating analysis of the off-exchange information environment into
predictive models significantly enhances forecast accuracy, as will be detailed in the subsequent
sections.

\subsubsection{Sentiment Analysis}
As noted in the Introduction, the emergence of the Transformer architecture has enabled deep learning
models to achieve significant progress in natural language understanding (NLU). This capability
is of particular value in finance, where traditional quantitative data alone prove insufficient
for precise forecasting.

Prior to the widespread adoption of modern language models for sentiment analysis (SA), various
approaches were employed --- LSTM--based networks, ULMFiT, autoregressive architectures, and others
\parencite{Hochreiter1997LSTM, howard2018ULMFIT}. For example, one study compared an autoregressive
model without sentiment features to an otherwise identical model augmented with sentiment inputs;
it demonstrated that in 77.8\% of cases the sentiment--aware version outperformed its purely
quantitative counterpart \parencite{NNAR2019}.

Modern pre-trained Transformer models, commonly referred to as large language models (LLMs),
open new horizons for financial forecasting. The most intuitive application is the SA of news.
This process requires the collection of a substantial corpus of texts, which are then annotated
with one of three labels: -1 (negative), 0 (neutral), or 1 (positive) \parencite{SA2020taxonomy}.

Manual annotation typically involves domain experts capable of assessing a text's impact on financial
markets. To ensure high data quality, the cross-consensus method \parencite{consensus1997bogdan}
is often applied: multiple experts independently label the same texts, and their labels
are reconciled. This methodology underpinned the creation of the widely used FinancialPhraseBank
dataset for fine-tuning models to financial sentiment analysis (FSA) tasks \parencite{Malo2014FPB}.

Algorithmic labeling methods based on asset price dynamics are less common, owing to their
subjectivity and instability. In particular, it is difficult to define a reliable pric--change
threshold for sentiment classification and to guarantee that observed gains are not driven
by extraneous factors.

Once annotation is complete --- a process consuming the majority of resources — pretrained language
models are fine-tuned by adjusting the weights of the final neural layers.

The SA taxonomy comprises three levels \parencite{SA2020taxonomy}:
\begin{itemize}
    \item \textbf{Document level.} Sentiment is evaluated across an entire document (e.g., a news
    article or report). This level assumes opinions pertain to a single entity, which seldom reflects
    reality. Furthermore, LLMs are technically constrained by maximum token counts and therefore cannot
    process multi-page filings (such as 10-K reports) or lengthy news articles in full. Recent advances
    have improved the handling of standard news articles, yet comprehensive reports remain beyond reach.
    \item \textbf{Sentence level.} Sentiment is assigned to individual sentences or short passages
    (typically 1--3 sentences), still under the assumption of a single subject. Most SA research operates
    at this level, supported by abundant datasets and well within the token processing capacity of modern
    LLMs. Common sources include news headlines and social-media posts on platforms such as X (formerly
    Twitter) and Reddit.
    \item \textbf{Aspect level (ABSA).} This level captures sentiment toward specific aspects of an entity.
    ABSA is the most advanced SA tier and is treated as a distinct field encompassing four subtasks: aspect
    extraction, aspect-polarity assignment, aspect-category detection, and category-polarity assignment.
    A specialized variant, targeted ABSA, permits multiple entities but restricts sentiment to one per entity.
    Further discussion of ABSA appears in \hyperref[sec:absa]{Section 1.1.3}.
\end{itemize}

Most SA methodologies require extensive text annotation, which represents a major drawback due to high
resource demands and inherent limitations:
\begin{itemize}
    \item No universal, precise definition of “sentiment” fits every application, forcing bespoke definitions
    for each task — a challenge given the diversity of textual patterns and contexts.
    \item Without significant investments in quality control (e.g., cross-consensus
    \parencite{consensus1997bogdan}), it is impossible to guarantee annotation reliability and objectivity,
    since human factors heavily
    influence label consistency.
\end{itemize}

As mentioned, SA's formulation may vary across domains. FSA is distinguished by its reliance not only
on textual data but also on the abundant quantitative information present in financial publications
\parencite{FSA2024techniques}.

Nevertheless, FSA encounters failures in several common scenarios, including: irrealis moods (conditional,
subjunctive or imperative moods), rhetorical devices (negative assertions, personification, sarcasm),
dependent opinions, unspecified aspects, out-of-vocabulary terms (jargon, microtext, named entities),
external references (allusions to knowledge not encoded in the model) \parencite{FSA2020problems}.

\subsubsection{Aspect Analysis}
\label{sec:absa}
<<To be done later>>