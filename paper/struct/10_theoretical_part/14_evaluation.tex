\sloppy  % Helps to alleviate overfull hbox warnings

\subsubsection{General Language Understanding Evaluation (GLUE)}

The General Language Understanding Evaluation (GLUE) benchmark constitutes a standardized framework for assessing the language comprehension
capabilities of natural language processing (NLP) models \parencite{wang2018GLUE}. It comprises 9 tasks encompassing classification, semantic
similarity evaluation, and textual entailment recognition. Through the diversity of these tasks, GLUE facilitates the identification
of modelsâ€™ ability to generalize and effectively transfer learned representations across a range of linguistic challenges.

The primary components of GLUE include:

\begin{itemize}
    \item \textbf{A suite of nine tasks}, each derived from pre-existing corpora and targeting distinct aspects of language
    understanding (e.g., linguistic acceptability, sentiment analysis, paraphrase detection).
    \item \textbf{A diagnostic dataset} for an in-depth evaluation of model performance in capturing various linguistic phenomena.
    \item \textbf{A public leaderboard and dashboard} that enable continuous tracking of benchmark performance and provide
    visualization of model results on the diagnostic tasks.
\end{itemize}

Below is a summary table of the key characteristics of the datasets included in the GLUE benchmark:

\input{tab/glue.tex}

In summary, the GLUE benchmark provides a robust foundation for evaluating both standard and domain-adapted NLP models.
Its comprehensive design and the inclusion of diverse linguistic tasks allow for a nuanced analysis of model capabilities.
Following this overview, the FLUE benchmark, which is tailored for the evaluation of models in the financial context,
will be discussed to further complement the assessment of domain-adaptive pre-training strategies.

\subsubsection{Financial Language Understanding Evaluation (FLUE)}

The Financial Language Understanding Evaluation (FLUE) benchmark is a domain-specific analog to the GLUE benchmark,
tailored specifically for the financial domain \parencite{FLANG2022FLUE}. This benchmark was developed very recently based
on 5 diverse datasets. Its creation was driven by the need to evaluate models capable of effectively processing
financial texts, as standard general-purpose datasets often fail to capture the unique characteristics of financial
lexicon and the specific tasks inherent to this domain.

\input{tab/flue.tex}

FLUE covers 5 distinct financial tasks, which allow for a comprehensive evaluation of model performance across various aspects
of financial language. The statistics presented in Table~\ref{tab:FLUE} demonstrate the scale and diversity of the included datasets.
Moreover, all datasets that comprise FLUE are characterized by low ethical risks and do not contain confidential information regarding
any organization or individual. In addition, explicit consent was obtained from the authors of each dataset prior to their inclusion
in the benchmark, underscoring its legitimacy and ethical soundness.

The emergence of the FLUE benchmark is driven by the necessity to standardize the evaluation of models in the field of financial
language understanding. The financial sector imposes unique requirements for processing textual data, such as high terminological
complexity, market dynamism, and specific tasks (e.g., sentiment analysis of news headlines, information extraction, etc.). These
factors have led to the creation of a heterogeneous set of tasks unified within FLUE, thereby enabling a holistic assessment
of different models. Thus, FLUE serves as an essential tool for researchers, facilitating objective model comparison
and the identification of areas for further improvement in financial NLP approaches.

In the context of this work, FLUE provides an objective benchmark for assessing model quality. However, despite the aforementioned
advantages of this benchmark, it still suffers from a general limitation: it is designed for models with a context window
of 512 tokens. Consequently, FLUE may not fully reveal the true potential of models that are capable of processing longer contexts
compared to the more constrained models of previous generations such as BERT, FinBERT, ELECTRA, and others.

\subsubsection{Clustering Evaluation Metrics}
Silhouette Index + Stability Index