\subsubsection{Models}
LSTM \parencite{Hochreiter1997LSTM}

<<...>>

BERT \parencite{devlin2019BERT}

<<...>>

FinBERT (2019 --- first) \parencite{Araci2019FinBERT}

<<...>>

FinBERT (2020 --- good) \parencite{Yang2020FinBERT, Huang2023FinBERT}

<<...>>

FinBERT (2020 --- best) \parencite{Liu2020FinBERT}

<<...>>

ModernBERT \parencite{Warner2024ModernBERT}

<<...>>

\subsubsection{Techniques}
\textbf{Domain-Adaptive Pretraining (DAPT).} --- \parencite{gururangan2020DAPT}

Based on the calculations, DAPT provides on average a 4\% increase in benchmarks on a relative scale compared to the baseline model,
which was not domain-specific. This figure is quite significant, considering that for some specific tasks the gains can be as high as 20\%
\parencite*{gururangan2020DAPT}.

\textbf{Fusion Mechanisms.} There are three commonly accepted mechanisms for multimodal data fusion.

The first, Early Fusion, involves feeding all features (stock quotes, technical indicators, and textual embeddings)
at once into a single CNN-LSTM model \parencite{Karpathy_2014_CVPR, dutt2022shared}. The advantage of the method
is the ease of implementation and the ability to immediately learn cross-modal dependencies. However, in practice
Early Fusion is prone to “choking” in the noise of one of the modalities and loses flexibility when dynamically
estimating the contribution of each modality \parencite{dutt2022shared}.

The second method, Late Fusion, combines the predictions of individual channels (each modality is processed by its
CNN-LSTM branch) only at the final stage \parencite{Karpathy_2014_CVPR, ortega2019multimodal}. This approach is characterized
by modularity (easy replacement or additional training of a single channel), but it excludes extraction of low-level
cross-modal patterns and requires training all branches separately, which entails a multiple increase in computational
resources \parencite{joze2020mmtm}.

The third, compromise mechanism, Slow Fusion, provides staged, “slow” merging of links at different layers of the network
\parencite{feichtenhofer2016convolutional, dutt2022shared}. Slow Fusion approaches Early Fusion for early merging, and Late Fusion
for late merging. The key advantages of the method are: the balance between the autonomous processing of each modality
and the possibility of taking into account their interaction, preserving the “purity” of low-level features and
the flexibility of setting the number and depth of integration stages \parencite{Karpathy_2014_CVPR}. The main disadvantages
are the difficulty of choosing the optimal fusion level and increased computational costs due to parallel branches
at early layers, but nevertheless, less than in Late Fusion.

\textbf{Approaches to representation document embedding.} [CLS] token and Mean-pooling

\subsubsection{Benchmarks}

The General Language Understanding Evaluation (GLUE) benchmark constitutes a standardized framework for assessing the language comprehension
capabilities of natural language processing (NLP) models \parencite{wang2018GLUE}. It comprises 9 tasks encompassing classification, semantic
similarity evaluation, and textual entailment recognition. Through the diversity of these tasks, GLUE facilitates the identification
of models' ability to generalize and effectively transfer learned representations across a range of linguistic challenges.

The primary components of GLUE include:

\begin{itemize}
    \item \textbf{A suite of nine tasks}, each derived from pre-existing corpora and targeting distinct aspects of language
    understanding (e.g., linguistic acceptability, sentiment analysis, paraphrase detection).
    \item \textbf{A diagnostic dataset} for an in-depth evaluation of model performance in capturing various linguistic phenomena.
    \item \textbf{A public leaderboard and dashboard} that enable continuous tracking of benchmark performance and provide
    visualization of model results on the diagnostic tasks.
\end{itemize}

Below is a summary table of the key characteristics of the datasets included in the GLUE benchmark:

\input{tab/glue.tex}

In summary, the GLUE benchmark provides a robust foundation for evaluating both standard and domain-adapted NLP models.
Its comprehensive design and the inclusion of diverse linguistic tasks allow for a nuanced analysis of model capabilities.
Following this overview, the FLUE benchmark, which is tailored for the evaluation of models in the financial context,
will be discussed to further complement the assessment of domain-adaptive pre-training strategies.

The Financial Language Understanding Evaluation (FLUE) benchmark is a domain-specific analog to the GLUE benchmark,
tailored specifically for the financial domain \parencite{FLANG2022FLUE}. This benchmark was developed very recently based
on 5 diverse datasets. Its creation was driven by the need to evaluate models capable of effectively processing
financial texts, as standard general-purpose datasets often fail to capture the unique characteristics of financial
lexicon and the specific tasks inherent to this domain.

\input{tab/flue.tex}

FLUE covers 5 distinct financial tasks, which allow for a comprehensive evaluation of model performance across various aspects
of financial language. The statistics presented in Table~\ref{tab:FLUE} demonstrate the scale and diversity of the included datasets.
Moreover, all datasets that comprise FLUE are characterized by low ethical risks and do not contain confidential information regarding
any organization or individual. In addition, explicit consent was obtained from the authors of each dataset prior to their inclusion
in the benchmark, underscoring its legitimacy and ethical soundness.

The emergence of the FLUE benchmark is driven by the necessity to standardize the evaluation of models in the field of financial
language understanding. The financial sector imposes unique requirements for processing textual data, such as high terminological
complexity, market dynamism, and specific tasks (e.g., sentiment analysis of news headlines, information extraction, etc.). These
factors have led to the creation of a heterogeneous set of tasks unified within FLUE, thereby enabling a holistic assessment
of different models. Thus, FLUE serves as an essential tool for researchers, facilitating objective model comparison
and the identification of areas for further improvement in financial NLP approaches.