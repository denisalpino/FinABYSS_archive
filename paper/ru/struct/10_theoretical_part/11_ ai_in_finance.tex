\subsubsection{Price Prediction}
There exists a variety of approaches demonstrating the effectiveness of asset price prediction using deep
neural networks. Recurrent neural networks (RNN) are undeniably the most prevalent for this task, while
convolutional neural networks (CNN) are often employed as auxiliary components.

The long short-term memory (LSTM) \parencite{Hochreiter1997LSTM} architecture is the most representative
member of the RNN family and is frequently applied to price forecasting. Its adaptations ---
such as bidirectional LSTM (Bi-LSTM) and hybrid LSTM+CNN models --- also exhibit high performance.
Furthermore, with the advent of the Transformer \parencite{vaswani2017attention} architecture,
research has increasingly focused on adapting it to the characteristics of time series
\parencite{wen2022transformers}.

Among recent experiments, the following are particularly noteworthy:
\begin{itemize}
    \item A repository demonstrating the potential of the Transformer architecture for Bitcoin price
    forecasting\footnote{URL: \url{https://github.com/baruch1192/-Bitcoin-Price-Prediction-Using-Transformers}};
    \item A study comparing the performance of Bi-LSTM, hybrid Bi-LSTM+CNN, and Transformer models
    in predicting IBM stock prices\footnote{URL: \url{https://github.com/JanSchm/CapMarket}};
    \item An LSTM-based trading bot designed to capture short-term profits during sideways price
    movements of the CGEN asset, which achieved a 4\% daily return on deposit in backtesting\footnote{URL: \url{https://github.com/roeeben/Stock-Price-Prediction-With-a-Bot}}.
\end{itemize}

Nevertheless, individual models --- whether LSTM-based or decision tree--based --- have limited
adaptability to changing market regimes and struggle to adjust to their dynamics \parencite{Vukovi2024}.
Although this limitation is evident, it is often underestimated in the literature. According
to the Efficient Market Theory (EMT) \parencite{emt1970fama}, asset prices incorporate all available
market information, which casts doubt on the feasibility of precise forecasting using only
historical quantitative indicators (OHLC data, trading volume, and classical technical indicators).

Moreover, most models fail to account for nuanced factors such as limit order book dynamics,
interactions with other trading algorithms, and qualitative off-exchange information. Recent
studies indicate that integrating analysis of the off-exchange information environment into
predictive models significantly enhances forecast accuracy, as will be detailed in the subsequent
sections.

\subsubsection{Sentiment Analysis}
As noted in the Introduction, the emergence of the Transformer architecture has enabled deep learning
models to achieve significant progress in natural language understanding (NLU). This capability
is of particular value in finance, where traditional quantitative data alone prove insufficient
for precise forecasting.

Prior to the widespread adoption of modern language models for sentiment analysis (SA), various
approaches were employed --- LSTM--based networks, ULMFiT, autoregressive architectures, and others
\parencite{Hochreiter1997LSTM, howard2018ULMFIT}. For example, one study compared an autoregressive
model without sentiment features to an otherwise identical model augmented with sentiment inputs;
it demonstrated that in 77.8\% of cases the sentiment--aware version outperformed its purely
quantitative counterpart \parencite{NNAR2019}.

Modern pre-trained Transformer models, commonly referred to as large language models (LLMs),
open new horizons for financial forecasting. The most intuitive application is the SA of news.
This process requires the collection of a substantial corpus of texts, which are then annotated
with one of three labels: -1 (negative), 0 (neutral), or 1 (positive) \parencite{SA2020taxonomy}.

Manual annotation typically involves domain experts capable of assessing a text's impact on financial
markets. To ensure high data quality, the cross-consensus method \parencite{consensus1997bogdan}
is often applied: multiple experts independently label the same texts, and their labels
are reconciled. This methodology underpinned the creation of the widely used FinancialPhraseBank
dataset for fine-tuning models to financial sentiment analysis (FSA) tasks \parencite{Malo2014FPB}.

Algorithmic labeling methods based on asset price dynamics are less common, owing to their
subjectivity and instability. In particular, it is difficult to define a reliable pric--change
threshold for sentiment classification and to guarantee that observed gains are not driven
by extraneous factors.

Once annotation is complete --- a process consuming the majority of resources — pretrained language
models are fine-tuned by adjusting the weights of the final neural layers.

The SA taxonomy comprises three levels \parencite{SA2020taxonomy}:
\begin{itemize}
    \item \textbf{Document level.} Sentiment is evaluated across an entire document (e.g., a news
    article or report). This level assumes opinions pertain to a single entity, which seldom reflects
    reality. Furthermore, LLMs are technically constrained by maximum token counts and therefore cannot
    process multi-page filings (such as 10-K reports) or lengthy news articles in full. Recent advances
    have improved the handling of standard news articles, yet comprehensive reports remain beyond reach.
    \item \textbf{Sentence level.} Sentiment is assigned to individual sentences or short passages
    (typically 1--3 sentences), still under the assumption of a single subject. Most SA research operates
    at this level, supported by abundant datasets and well within the token processing capacity of modern
    LLMs. Common sources include news headlines and social-media posts on platforms such as X (formerly
    Twitter) and Reddit.
    \item \textbf{Aspect level (ABSA).} This level captures sentiment toward specific aspects of an entity.
    ABSA is the most advanced SA tier and is treated as a distinct field encompassing four subtasks: aspect
    extraction, aspect-polarity assignment, aspect-category detection, and category-polarity assignment.
    A specialized variant, targeted ABSA, permits multiple entities but restricts sentiment to one per entity.
    Further discussion of ABSA appears in \hyperref[sec:absa]{Section 1.1.3}.
\end{itemize}

Most SA methodologies require extensive text annotation, which represents a major drawback due to high
resource demands and inherent limitations:
\begin{itemize}
    \item No universal, precise definition of “sentiment” fits every application, forcing bespoke definitions
    for each task — a challenge given the diversity of textual patterns and contexts.
    \item Without significant investments in quality control (e.g., cross-consensus
    \parencite{consensus1997bogdan}), it is impossible to guarantee annotation reliability and objectivity,
    since human factors heavily
    influence label consistency.
\end{itemize}

As mentioned, SA's formulation may vary across domains. FSA is distinguished by its reliance not only
on textual data but also on the abundant quantitative information present in financial publications
\parencite{FSA2024techniques}.

Nevertheless, FSA encounters failures in several common scenarios, including: irrealis moods (conditional,
subjunctive or imperative moods), rhetorical devices (negative assertions, personification, sarcasm),
dependent opinions, unspecified aspects, out-of-vocabulary terms (jargon, microtext, named entities),
external references (allusions to knowledge not encoded in the model) \parencite{FSA2020problems}.

\subsubsection{Aspect Analysis}
\label{sec:absa}
ABSA is a specialized task of identifying and assessing sentiment in relation to specific “aspects” ---
characteristics or properties of the object under consideration. In the financial sphere, such aspects
can be risk factors, credit policy, macroeconomic indicators and other elements mentioned in publications.
The classical approach to ABSA includes three steps: selection of aspects from the text, determination
of sentiment polarity for each aspect, and aggregation of results to build a final picture of expert
or market opinions.

Parallel to the development of ABSA, topic modeling has been actively developing in the linguistic community,
with the main goal of identifying hidden “topics” in document corpora. One of the first and most influential
methods was Latent Dirichlet Allocation (LDA) \parencite{LDA2003}, where topics are formulated as word
distributions, and documents are considered as mixtures of such distributions. In the context of financial
texts, LDA allows to identify the main areas of discussion (e.g., “monetary policy”, “corporate risks”, etc.),
but has difficulties in modeling syntagmatic relations and dynamic vocabulary.

At the intersection of embedding models and topic analysis, the idea of top2vec \parencite{angelov2020top2vec}
emerged, where topics are represented in the same vector space as words, thus combining the advantages
of distributed representations and topic structures. Further evolution led to BERTopic \parencite{BERTopic2022},
where density-based algorithms over embeddings are used to build topics, and then the most characteristic keywords
are extracted for each cluster. This model shows high adaptability to lexical changes and allows handling dynamic,
even multi-lingual corpora.

The connection between ABSA and topic modeling is obvious: aspects are, broadly speaking, the topics against which
sentiment is measured. In traditional ABSA, aspects are specified or extracted based on linguistic patterns (e.g.,
rules on dependencies or dictionaries), whereas topic modeling opens the way to automatically detect aspects as latent
latent variables. The advantage of this approach in the financial domain is that a previously unknown set of aspects
(“topics”) can be extracted without manual partitioning, after which a standard sentiment evaluation procedure (e.g.,
using LLM) is applied to the identified clusters of documents, resulting in a more complete and interpretable analysis
of market opinions.

Thus, this work treats aspects as topics in topic modeling. And the paper proposes a method of initial clustering
of vector representations that forms “topic clusters” in order to aggregate sentiment across them. This aggregation
allows us to (1) identify both global trends and local, niche aspects of the financial market, (2) minimize the need
for manual aspect partitioning, and (3) ensure interpretability of the results by explicitly linking sentiment
to themes expressed through the key terms of each cluster. This combination took the best of both directions: topic
structure under LDA/topic2vec/BERTopic and polarity precision under ABSA.