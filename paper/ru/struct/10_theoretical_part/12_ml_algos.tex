\subsubsection{Понижение размерности}
\textbf{Метод главных компонент (Principal Component Analysis, PCA)} является линейным методом
понижения размерности (dimensionality reduction, DR), основанным на разложении ковариационной
матрицы исходных данных. При нулевом среднем по признакам он находит собственные векторы
ковариации (компоненты), отвечающие за максимальную дисперсию:
$\Sigma = X^T X$, $\Sigma w_i = \lambda_i w_i$, проекцию данных $Y = XW_{d'}$ на первые
$d'$ собственных векторов.

PCA стремится сохранить как можно больше дисперсии исходных данных и, как следствие, хорошо
сохраняет «глобальную» структуру кластеров \parencite{TRIMAP2019}. На практике PCA часто
применяется как промежуточный этап: например, при работе с высокоразмерными текстовыми
эмбеддингами (несколько сотен признаков, например, BERT-эмбеддинги размерности 768) перед нелинейными
методами понижения размерности. Использование PCA позволяет значительно сократить размерность
(до десятков --- сотен компонент) и ускорить последующие вычисления \parencite{huang2022towards}.

Однако PCA --- линейный метод, и он не отражает более сложных нелинейных зависимостей, присущих
текстовым эмбеддингам. Вследствие этого тонкие локальные отношения слов и документов могут
быть потеряны, хотя общая структура («глобальный ландшафт» данных) при этом чаще всего сохраняется.


However, PCA is a linear method, and it does not capture the more complex nonlinear dependencies
inherent in language embeddings. As a consequence, subtle local word-document relationships
may be lost, although the overall structure (the “global landscape” of the data) is most often
preserved.

\textbf{Стохастическое вложение соседий с t-распределением (t-distribution Stochastic Neighbor Embedding,
t-SNE)} нелинейный стохастический метод, ориентированный на сохранение локальной структуры данных. В
высокоразмерном пространстве он вычисляет условные вероятности
$p_{j|i} \propto \exp(-|x_i-x_j|^2/2\sigma_i^2)$, отражающие близость соседей; затем симметризует их:
$p_{ij}=(p_{j|i}+p_{i|j})/2n$. В низкоразмерном отображении задаётся похожая мера (распределение Стьюдента
с 1 степенью свободы). Алгоритм оптимизирует расстановку точек $Y$, минимизируя дивергенцию
Кульбака–Лейблера $C = \sum_{i,j} p_{ij}\ln\frac{p_{ij}}{q_{ij}}$. В результате в низкоразмерном пространстве
у близких в исходном пространстве точек будет сохраняться локальная кластерная структура.

Гиперпараметр перплексии определяет число эффективных соседей. t-SNE показывает впечатляющую визуализацию
локальных кластеров, но плохо воспроизводит глобальное расстояние между кластерами. Он вычислительно дорог
при больших выборках и обычно используется лишь для финального перехода в двумерное пространство (или трехмерное),
а не для промежуточного снижения размерности. Для текстовых эмбеддингов t-SNE часто применяют после
предварительной обработки (например, снижения размерности PCA), поскольку напрямую на несколько сотен
признаков он масштабируется плохо.

\textbf{Равномерная аппроксимация и проекция многообразий (Uniform Manifold Approximation and Projection, UMAP)}
метод нелинейного DR на основе предположения о многообразии. Теоретически UMAP опирается на риманову
геометрию и теорию «нечетких симплициальных множеств» (fuzzy simplicial sets). Алгоритм строит граф
$k$-ближайших соседей в исходном пространстве, затем каждой паре точек присваивает «членство» в нечетком
множестве по формуле вида:

\begin{equation}
    \mu_{ij}=\exp\big(-\cfrac{\max(0, d(x_i, x_j) - \rho_i)}{\sigma_i}\big),
\end{equation}

где $\rho_i$ учитывает плотность соседей. После чего эти локальные симплициальные множества объединяются
и симметризуются, получается взвешенный граф данных. Далее строится аналогичный «нечеткий» граф в низкоразмерном
пространстве и оптимизируется расположение точек, минимизируя энергию перекрёстной энтропии между двумя графами.

UMAP удерживает локальную структуру данных, при этом стремясь равномерно распределить точки на многообразии;
в отличие от t-SNE, он может лучше сохранять некоторые глобальные особенности (обусловлено используемой
кросс-энтропией). Основные гиперпараметры UMAP – число соседей ‘n\_neighbors’ (задаёт масштаб локальности)
и ‘min\_dist’ (минимальное расстояние точек в отображении).

UMAP демонстрирует высокую скорость и масштабируемость (метод может работать сразу на произвольном числе
выходных измерений). В эксперименте было показано, что UMAP даёт качество визуализации, сопоставимое или лучше,
чем у t-SNE, при существенно меньшем времени работы \parencite{UMAP2018mcinnes}. К преимуществам UMAP можно
отнести также сохранение более крупномасштабной структуры данных и возможность понижать размерность вплоть
до многомерных векторов, а не только двумерных.

Тем не менее, UMAP может некорректно отображать сильно разреженные кластеры, «выравнивая» плотные и разреженные
области (алгоритм фактически стремится к равномерному распределению данных на предполагаемом многообразии).
Кроме того, результаты UMAP чувствительны к выбору гиперпараметров и степени шума в выборке (алгоритм использует
приближённый поиск соседей и сэмплирование отрицательных примеров) \parencite{huang2022towards}. Впрочем, в
большинстве прикладных задач по кластеризации текстовых данных UMAP показал себя устойчивым и эффективным
инструментом.

Существует несколько Python-реализаций данного метода. Классическая реализация --- 'umap-learn' (CPU) ---
имеет крайне широкое распространение \parencite{mcinnes2018umap-software}. Для ускорения процесса обучения
существует GPU-реализации ('cuML') \parencite{cuml2020machine}. GPU-реализация UMAP в 'cuML' может давать
ускорение до 10-100× по сравнению с CPU-версией на больших объёмах данны. Однако в ранних версиях 'cuML'
в целях скорости были введены приближения, что иногда влечёт небольшую разницу в качестве отображения
по сравнению с оригиналом. Также GPU реализация UMAP может понизить сохранность локальной структуры
относительно эталонной версии. Так, 'cuML' UMAP подходит для очень больших объемов данных, а реализация
из 'umap-learn' более универсален и стабилен, но работает медленно медленнее.



\textbf{Триплетная аппроксимация и проекция многообразий (Triplet Manifold Approximation and Projection, TriMAP)}
метод обучения вложения с упором на глобальную структуру данных \parencite{TRIMAP2019}. Он формулирует задачу
через тройки точек $(i,j,k)$: «точка $i$ должна быть ближе к $j$, чем к $k$» в низкоразмерном представлении.
Выбор таких триплетов базируется на ближайших и дальних соседях в исходном пространстве; каждому триплету придаётся
вес, отражающий относительную близость пар в исходном пространстве. Оптимизация выполняется по большой выборке
информативных триплетов с помощью градиентного спуска.

TriMAP сохраняет глобальную структуру гораздо лучше, чем t-SNE и часто лучше, чем UMAP. Также TriMAP хорошо
масштабируется и демонстрирует малое время работы для больших и высокоразмерных выборок \parencite{TRIMAP2019}.
С точки зрения текстовых эмбеддингов TriMAP может дать более читаемую картину расположения кластеров документов
на двумерную плоскость (хотя при этом детали локальных сообществ могут сглаживаться).

\textbf{Аппроксимация парных управляемых многообразий (Pairwise Controlled Manifold Approximation and Projection,
PaCMAP)} более новый метод, специально сконструированный для баланса локальной и глобальной структуры
\parencite{PACMAP2021}. Как и в TriMAP, здесь используются выборки пар точек разных типов: «близкие» пары (соседи),
«средние» пары (между кластерами) и «дальние» пары. Для каждого типа пар заданы соответствующие веса и силы
притяжения/отталкивания. В результате оптимизируемая функция потерь стремится одновременно сжимать локально близкие
точки и раздвигать удалённые, сохраняя глобальную форму распределения.

PaCMAP устойчив к выбору гиперпараметров и сокращению размерности при предобработке, хорошо сохраняет как локальную,
так и глобальную структуру. Недостатки PaCMAP --- относительная новизна и необходимость подбора долей разных типов пар.

Систематические сравнения указывают на характерное разделение свойств этих алгоритмов. Так, PCA, TriMAP и PaCMAP
хорошо сохраняют глобальные расстояния (крупномасштабную структуру кластеров), в то время как t-SNE и UMAP лучше
улавливают локальные детали \parencite{huang2022towards}. PCA традиционно используется для предобработки: снижение
размерности до десятков компонент ускоряет дальнейший анализ и делает его более стабильным. Однако заметно, что полная
предобработка PCA может исказить исходные расстояния, поэтому результаты итогового вложения (например, визуализации
t-SNE/UMAP) часто зависят от числа компонент PCA. В экспериментальных оценках метода PaCMAP и TriMAP показали
наилучшее согласование глобальных расстояний, тогда как UMAP и t-SNE в среднем уступали им в этой задаче.
И наоборот, в задачах классификации на векторных признаках (проверка локальной согласованности) лучше всего
проявили себя t-SNE и UMAP.

В целом выбор метода понижения размерности для высокоразмерных эмбеддингов документов зависит от задачи:
для последующей кластеризации и тематического часто применяют PCA или UMAP (для более «стабильного» представления
кластеров), а для финальной двумерной визуализации и детального анализа локальных кластеров --- t-SNE, UMAP или PaCMAP.
Тщательный выбор гиперпараметров и, возможно, сочетание методов (например, PCA+UMAP) позволяет добиться лучшего
отображения структуры текстовых данных.

\subsubsection{Кластеризация}
В рассматриваемой схеме тематического анализа финансовых новостных статей после получения текстовых
эмбеддингов применяется алгоритм понижения размерности. После такого сокращения размерности алгоритмы
кластеризации работают в менее разреженном пространстве, что может улучшить выделение плотных областей
и снизить влияние шумовых факторов.

\textbf{K-Средних} один из классических партиционных алгоритмов кластеризации \parencite{kmeans2010data}.
Он ищет разбиение данных на $K$ кластеров, минимизируя внутрикластерный разброс точек. Обозначим кластеры
$C_1,\dots,C_K$ и центроиды кластеров $\mu_k$; тогда оптимизируемая целевая функция (метрика «суммы
квадратов расстояний» от точек до соответствующих центроидов) задаётся как

\begin{equation}
    J(C)= \sum_{k=1}^K \sum_{x_i \in C_k} \|x_i - \mu_k\|^2,
\end{equation}

и минимизируется при разбиении по Евклидовой метрике. Нахождение глобального минимума этой функции является
NP-полной задачей, поэтому метод K-Средних выполняет жадную итеративную процедуру переклассификации точек
и пересчёта центроидов (обычно со случайной инициализацией), которая сходится к локальному минимуму. Важной
особенностью K-Средних является требование заранее задать число кластеров $K$ и начальные приближения.

Поскольку алгоритм обычно использует Евклидову метрику, он образует в основном сферические кластеры.
Все объекты автоматически относятся к какому-либо кластеру (жёсткое «принадлежность» каждой точки
одному кластеру), и алгоритм не выделяет явно выбросы или шум. Среди преимуществ метода --- простота
реализации, низкие вычислительные затраты и широкая распространённость. Однако K-Средних неустойчив
к выбросам, и плохо выделяет вложенные или сильно неоднородные по форме кластеры.

\textbf{Пространственная кластеризация приложений с шумом на основе плотности (Density-Based Spatial
Clustering of Applications with Noiseб, DBSCAN)} метод кластеризации на основе плотности \parencite{DBSCAN1996}.
Он определяет кластеры как регионы высокой плотности данных, отделённые низкоплотными областями. Алгоритм использует
два параметра: радиус $\epsilon$ и минимальное число точек $min_{pts}$. Точка называется «ядром» кластера, если
в её $\epsilon$-окрестности содержится по крайней мере $min_{pts}$ точек. Точки, достижимые по плотности
от ядра, принадлежат тому же кластеру.

DBSCAN автоматически отделяет точки, не попавшие в плотные области, как шум, и не требует задания числа кластеров.
Благодаря этому метод находит кластеры произвольной формы и хорошо подходит для наборов данных с неравномерно
распределёнными объектами. Однако DBSCAN имеет значимые ограничения: выбор единого порога $\epsilon$ критичен,
и при совмещении кластеров разной плотности алгоритм либо объединяет их в одно целое, либо разбивает на слишком
мелкие фрагменты. Кроме того, с ростом размерности пространства данные разрежаются, и отличить высокоплотную
область от низкоплотной становится затруднительно. Вследствие этого DBSCAN в высокоразмерных эмбеддингах текста
часто демонстрирует пониженную эффективность. Также сложность классического DBSCAN при отсутствии оптимизации
может достигать $O(n^2)$, хотя практические реализации с индексами обычно имеют существенно меньшую скорость работы.

\textbf{Иерархический DBSCAN (HDBSCAN)} иерархическое расширение метода DBSCAN \parencite{HDBSCAN2013}. В отличие
от DBSCAN, HDBSCAN не требует фиксированного $\epsilon$: вместо этого он рассчитывает расстояние взаимной
достижимости между точками $x$ и $y$ как

\begin{equation}
    d_{\mathrm{mreach}}(x,y) = \max\bigl\{d_{\mathrm{core}}(x),\,d_{\mathrm{core}}(y),\,d(x,y)\bigr\},
\end{equation}

где $d_{\mathrm{core}}(x)$ — расстояние от точки $x$ до её $k$-го ближайшего соседа (то есть минимальное
$\epsilon$, при котором $x$ становится «ядром» кластера). Затем строится граф полных взаимных достижимости
(или напрямую минимальное остовное дерево на таких расстояниях). Удаляя рёбра этого дерева в порядке убывания
веса, алгоритм формирует древовидную иерархию кластеров, отражающую вложенную структуру данных при разных
порогах плотности. Из этого дерева по критерию стабильности выделяются финальные кластеры. Такая процедура
эквивалентна проведению множества запусков DBSCAN при всех возможных $\epsilon$ и выбору наиболее значимых
«стабильных» групп. Важной чертой HDBSCAN является то, что он сам находит оптимальное число кластеров, требуя
лишь указать минимальный размер кластера ('min\_cluster\_size').

Благодаря иерархическому подходу HDBSCAN способен выявлять вложенные кластеры различной плотности и более
гибко адаптироваться к распределению данных по сравнению с DBSCAN и K-Средних. Алгоритм устойчив к флуктуациям
плотности: если в данных имеются области разной однородности, HDBSCAN выделит крупные разреженные кластеры
и более мелкие плотные кластеры одновременно. Он автоматически помечает выбросы, аналогично DBSCAN, но без
жёсткой привязки к одному порогу. Из-за дополнительной обработки (поиск $k$-ближайших соседей, построение MST
и анализ иерархии) HDBSCAN несколько сложнее вычислительно, но современные реализации с эффективными
структурами соседства обычно обеспечивают сопоставимую или даже лучшую производительность
\parencite{HDBSCAN2017software}. В целом HDBSCAN предоставляет более богатое описание структуры данных
на разных уровнях детализации и чаще даёт более осмысленные кластеры в сложных многомерных пространствах.

Существующие экспериментальные исследования показывают, что в задаче тематического анализа длинных текстов
каждый метод имеет свои плюсы и минусы. K-Средних часто используется как базовый метод ввиду простоты
и масштабируемости, однако он даёт сравнительно грубое разбиение тем, так как ограничен сферической формой
кластеров и требует заранее указать число тем. DBSCAN позволяет выявлять кластеры произвольной формы
и отделять шум, но его эффективность снижается на высокоразмерных эмбеддингах текстов из-за разреженности
и необходимости настройки глобального порога плотности.

Во многих сравнительных экспериментах показано, что HDBSCAN превосходит оба упомянутых метода по качеству
тематических кластеров: он автоматически адаптируется к вариативности плотности эмбеддингов, выявляет вложенные
темы разной детальности и надёжно исключает нерелевантный шум \parencite{HDBSCAN2017software, HDBSCAN2013}.
Эти выводы подтверждаются практическими применениями (например, при кластеризации новостных статей),
где HDBSCAN чаще всего даёт более интерпретируемые и стабильные результаты по сравнению с K-Средних
или «плоским» DBSCAN \parencite{BERTopic2022}.

\subsubsection{Метрики оценивания}
В задачах совместной оптимизации алгоритмов понижения размерности и кластеризации критически важно применять
метрики, способные одновременно оценивать качество проекции и чистоту выделенных групп. Две наиболее
распространённые метрики в этом контексте --- коэффициент силуэта \parencite{silouette1987} и индекс DBCV \parencite{dbcv2014density}.

Коэффициент силуэта характеризует для каждого объекта соотношение средней внутрикластерной и ближайшей
внекластерной дистанций \parencite{silouette1987}. Для объекта $i$ вычисляются

\begin{equation}
    a(i)=\frac{1}{|C_i|-1}\sum_{j\in C_i\setminus\{i\}}d(i,j),\quad b(i)=\min_{C\neq C_i}\frac{1}{|C|}\sum_{j\in C}d(i,j),
\end{equation}

где $C_i$ --- кластер, содержащий $i$, а $d(\cdot,\cdot)$ --- метрика (обычно Евклидова). Тогда

\begin{equation}
    s(i)=\frac{b(i)-a(i)}{\max\{a(i),\,b(i)\}}\in[-1,1].
\end{equation}

Среднее значение $s=\frac1N\sum_i s(i)$ отражает, насколько объекты внутри кластеров компактны и удалены
от соседних кластеров.

Коэффициент силуэта хорошо подходит для оценки разделимости при «сферических» кластерах, однако при плотностном
разделении (разной плотности и формы кластеров) он может дать завышенные или искажённые оценки, поскольку
при этом внутрикластерная и межкластерная дистанции не отражают качества плотностных областей. В частности,
то же справедливо и для других индексов подобных метрик, например, индекс Дэвиса–Болдена и Калински–Харабаша
\parencite{mmj2023liu}. Все они опираются на средние расстояния до центров или дисперсию по кластерам и
не учитывают неоднородность плотности и выделение шума \parencite{liu2024newindexclusteringevaluation}.

Так, была разработана метрика на основе плотностей специально для DBSCAN/HDBSCAN-кластеризации --- индекс валидации
кластеризации на основе плотности (Density-Based Clustering Validation Index, DBCV) \parencite{dbcv2014density}.
Данная метрика измеряет среднее отношение плотностей «внутри --- между» кластерами, а также явно обрабатывает
шум и произвольную форму кластеров. В совокупности, она позволяет корректно оценивать кластеры произвольных
форм, которые строго говоря называются нерегулярными.

С другой стороны, у DBCV есть и ключевой недостаток --- при перекрытии кластеров он дает большую оценку их
объединению, нежели рассечению, что может негативно сказаться именно на иерархической плотностной кластеризации.
Еще одна проблема с существующими индексами валидности кластера заключается в предположении, что данные внутри
кластера имеют однородное распределение, даже если форма кластера произвольна

Также существуют и другие метрики, например, такие, как VIASCKDE \parencite{viasckde2022} и Min-Max-Jump
Silhouette coefficient (MMJ-SC) \parencite{mmj2023liu, liu2024newindexclusteringevaluation},
они явно обладают потенциалом в достаточно широком спектре задач, однако все равно уступают DBCV в стабильности
на большом спектре различных задач и тестовых кейсов. Более того, например, MMJ-SC абсолютно новая метрика и еще
не была досаточно протестировна на практике.

На тестовых данных кластеров сложных вложенных форм в трехмерном пространстве, DBCV индекс, единственная метрика,
которая корректно отработала, показав максимальное значение 0.53 на истинном варианте кластеризации, в то же время
коэффициент силуэта хоть и показал большее максимальное значение 0.62, но данный максимум метрики пришелся
на категорически неверную разметку кластеров \parencite{liu2024newindexclusteringevaluation}.