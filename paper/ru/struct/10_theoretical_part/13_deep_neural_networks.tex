\subsubsection{Models}

Initially, classical recurrent \parencite{RNN1986} and convolutional \parencite{CNN1998lecun} networks
have been applied in financial text analytics. RNNs, including improved variants of LSTM \parencite{Hochreiter1997LSTM},
are able to process text as a sequence, modeling the context of a word based on previous states. The turning
point was the transition to transformers \parencite{vaswani2017attention}.

\textbf{Bidirectional Transformer Encoder Representations (BERT)}. The BERT model \parencite{devlin2019BERT} proposed
a bidirectional encoder based on attention mechanisms that allowed the context of a word to be considered
simultaneously from the left and right. BERT sets a new paradigm: the model is pre-trained on large corpora
on a masked language modeling (MLM) task and then fine-tuned for specific tasks.

The key architectural feature of BERT is a fully-connected transformer model with a Multi-Head Attention
mechanism and hidden Feed-Forward layers (BERT-base has 12 layers of 768 neurons each). At the same time,
BERT is basic limited by the length of the input sequence (up to 512 tokens), which puts limitations
on the processing of long documents. However, the BERT architecture has proven to be extremely successful
for fine-tuning: it has proven itself in classification, question-answer systems, and other tasks with
minimal changes to the output layers.

\textbf{Financial BERT (FinBERT)}. Adapted versions of BERT, FinBERT, have been developed for the financial
domain. Thus, in 2019, a FinBERTv1 [Araci (2019)] model was developed, a pre-trained BERT on the financial
dataset for the tone analysis task. His model showed improvement in all metrics. FinBERTv1 uses the standard
BERT architecture but has a specific vocabulary of financial texts --- e.g., terms from reports and news.
Despite a smaller training sample size than general-purpose models, FinBERTv1's success is primarily due
to the fact that the model “understands” the context of financial terminology better than general-purpose
models.

Later, another version of FinBERTv2 was developed by specifically pre-training it on financial corpora
(news, company reports, financial forums) and using multi-task learning \parencite{Liu2020FinBERT}. This
allowed FinBERTv2 to learn rich representations tailored to specific financial vocabulary. FinBERTv2
performs markedly better on financial sentiment tasks and question-answer systems.

Another model, FinBERTv3 \parencite{Yang2020FinBERT, Huang2023FinBERT}, also adapted BERT to finance, but with a focus
on interpretability and extracting information from analytical reports. The authors develop the idea
of context-awareness: their FinBERTv1 “integrates financial knowledge” and better “summarizes contextual
information in financial texts”. In doing so, the model significantly outperforms both dictionary-based
approaches and classical ML algorithms, as well as convolutional and LSTM models on sentiment classification
tasks in analyst markup experiments. FinBERTv3 \parencite{Yang2020FinBERT} has a particularly strong
advantage in handling sentences where other models erroneously label “neutral”-thanks to better
context sensing, the model detects positive/negative connotations where shallow methods miss.

Thus, all three FinBERT variants are based on the BERT Transformer architecture, but differ in their training
strategy: the FinBERTv1 model is simply fine-tuned on a specialized dataset, FinBERTv2 is pre-trained on mixed
data and tasks, and FinBERTv3 emphasizes financial information extraction, contextual analysis, and interpretation.
In all cases, financial specialization gives superiority over the “pure” BERT model in the financial domain.

\textbf{ModernBERT} is the latest generation of model-based encoders, this architecture retains the idea of BERT but
includes several enhancements \parencite{Warner2024ModernBERT}. First, ModernBERT is trained on a very large amount of data ($\approx$3 trillion
tokens) and initially supports sequence lengths up to 8192 tokens, while the original BERT is limited to 512
tokens. This qualitatively changes the model's abilities --- it can capture a long document context in its
entirety without special post-processing. Second, the ModernBERT architecture incorporates technological
innovations: for example, it employs GeGLU gated layers, uses Rotational Position Encoding (RoPE) for better
position encoding over long distances, and introduces an alternating local-global attention mechanism instead
of the full attention mechanism of BERT. This allows longer texts to be processed more efficiently in terms
of computational resources. In addition, ModernBERT is optimized for speed and memory: it is designed with GPU
hardware features in mind.

Compared to BERT and its “canonical” variants, ModernBERT achieves new performance records. For example,
it outperforms BERT on all standard GLUE benchmark tasks (see Section \ref{sec:benchmarks}) and significantly
improves performance on long context retrieval and search tasks. Its performance is significantly better than
its competitors on long context tasks. For example, when building ColBERT indexes, ModernBERT has a metric
of 80.2, compared to only 28.1 for BERT and 69.3 for the previous model with the same context
\parencite{Warner2024ModernBERT,devlin2019BERT}. At the same time, ModernBERT also processes short contexts
faster than the previous leading models while saving memory. In other words, ModernBERT represents
a “new generation” of encoders. Not only is it efficient, but it is specifically tailored for long texts
and large data sizes.

Thus, ModernBERT is an evolutionary step in the encoder architecture. It retains the concept of the BERT
bidirectional transformer, but adapts it to large-scale data and long sequences. By combining new architectural
techniques and a huge training corpus, ModernBERT significantly expands the processing capabilities
of large-volume financial (and any other) texts compared to the original BERT.

\subsubsection{Techniques}
\textbf{Fine-tuning.} In modern transformer-based NLP systems, the pre-trained model serves
as a generalized linguistic kernel, but it is not always ready to provide optimal embedding
representations for specific tasks. Fine-tuning is an additional learning phase in which
the already pre-trained model adjusts its internal weights for narrowly targeted goals.
Fine-tuning leverages the labeled data specifically for the target task (classification,
regression, etc.). In financial topic modeling, fine-tuning is critical to ensure that
the model does not simply generate universal representations of text, but emphasizes
relevant patterns, intensity of financial terms, syntactic constructions, and even
quantitative mentions to ensure greater accuracy in clustering and predictive modules.

One of the most popular downstream tasks is Semantic Textual Similarity (STS) ---
determining the semantic similarity of two text fragments. Unlike binary classification
tasks, STS is formulated as a regression task: the model needs to produce a continuous
similarity score, usually in the range of 0 to 5 \parencite{Cer2017STSB} or 0 to 1
\parencite{gao2021simcse}.

The STS problem places high demands on the quality of embeddings: they must preserve
subtle semantic nuances while being invariant to syntactic permutations and style
variations. Thus, a Sentence-BERT architecture was proposed, where two BERT-network
instances exchange gradients via a Siamese configuration and their output vectors are
compared by cosine distance, which has significantly accelerated and improved STS
\parencite{SBERT2019} estimation.

Thus, fine-tuning the embedding model on STS plays a key role in constructing a payplane
where DR and clustering models are combined. Fine-tuning ensures
embeddings in which local and global semantic relations are aligned in such a way that
both nonlinear DR preserves relevant neighborhoods and density
clustering automatically extracts topic associations without excessive noise.

\textbf{Domain-Adaptive Pretraining (DAPT).}  In current LLM practice, one-time pre-training
on extensive corpus-wide data is proving to be insufficient. DAPT is an additional phase
of pre-training on target domain texts, preceding the fine-tuning phase on a particular
task \parencite{gururangan2020DAPT}.

Unlike classical fine-tuning, in which the model merely adjusts the weights in the last
layers to fit a particular dataset and task, DAPT extends the original language modeling
task by allowing the model to “refresh” and deepen its understanding of the lexicon, syntax,
and semantics of a specific domain --- e.g., financial publications. This incremental build-up
of pre-training demonstrates significant quality gains on domain-specific tasks both when
marked-up data is scarce and when it is abundant.

Based on the calculations, DAPT provides on average a 4\% increase in benchmarks on a relative
scale compared to the baseline model, which was not domain-specific. This figure is quite significant,
considering that for some specific tasks the gains can be as high as 20\% \parencite{gururangan2020DAPT}.

The need for domain adaptation is particularly evident in the financial sector: generic corpora
such as BookCorpus \parencite{BookCorpus2015} or Wikipedia \parencite{WikiText2017} do not
capture the terminology, metaphors and stylistic patterns of press releases, analyst reports
and stock news. Numerous studies have confirmed the advantages of DAPT in specialized fields.
For example, SciBERT developers have demonstrated that pre-training BERT on a corpus
of scientific articles leads to improved performance on the tasks of scientific entity
extraction and article classification by discipline, providing up to 2-3 points increase
in F1 metrics compared to the baseline \parencite{SciBERT2019} model. Similarly, BioBERT adapted
to biomedical text improved the accuracy of extracting medical terms and relations by 10-20\%
compared to the baseline BERT \parencite{BioBERT2020}.

In the financial domain, domain adaptation allows LLMs to better capture the nuances of mentions
of stocks, interest rates and regulatory terms, which together with the topic clustering task
improves the quality of embeddings and makes the output representations more “legible” in subsequent
clustering by HDBSCAN. Thus, DAPT acts as a “bridge” between general language knowledge and the needs
of the subject domain, allowing for a significant improvement in results on domain-specific tasks.

\textbf{Fusion Mechanisms.} In multimodal and multicomponent systems, the fusion mechanism is responsible
for combining information from different sources or representations, enabling joint processing
of heterogeneous signals. There are three commonly accepted mechanisms for multimodal data fusion.

The first, Early Fusion, involves feeding all features (stock quotes, technical indicators, and textual embeddings)
at once into a single CNN-LSTM model \parencite{Karpathy_2014_CVPR, dutt2022shared}. The advantage of the method
is the ease of implementation and the ability to immediately learn cross-modal dependencies. However, in practice
Early Fusion is prone to “choking” in the noise of one of the modalities and loses flexibility when dynamically
estimating the contribution of each modality \parencite{dutt2022shared}.

The second method, Late Fusion, combines the predictions of individual channels (each modality is processed by its
CNN-LSTM branch) only at the final stage \parencite{Karpathy_2014_CVPR, ortega2019multimodal}. This approach is characterized
by modularity (easy replacement or additional training of a single channel), but it excludes extraction of low-level
cross-modal patterns and requires training all branches separately, which entails a multiple increase in computational
resources \parencite{joze2020mmtm}.

The third, compromise mechanism, Slow Fusion, provides staged, “slow” merging of links at different layers of the network
\parencite{feichtenhofer2016convolutional, dutt2022shared}. Slow Fusion approaches Early Fusion for early merging, and Late Fusion
for late merging. The key advantages of the method are: the balance between the autonomous processing of each modality
and the possibility of taking into account their interaction, preserving the “purity” of low-level features and
the flexibility of setting the number and depth of integration stages \parencite{Karpathy_2014_CVPR}. The main disadvantages
are the difficulty of choosing the optimal fusion level and increased computational costs due to parallel branches
at early layers, but nevertheless, less than in Late Fusion.

\subsubsection{Benchmarks}
\label{sec:benchmarks}
The General Language Understanding Evaluation (GLUE) benchmark constitutes a standardized framework for assessing the language comprehension
capabilities of natural language processing (NLP) models \parencite{wang2018GLUE}. It comprises 9 tasks encompassing classification, semantic
similarity evaluation, and textual entailment recognition. Through the diversity of these tasks, GLUE facilitates the identification
of models' ability to generalize and effectively transfer learned representations across a range of linguistic challenges.

The primary components of GLUE include:

\begin{itemize}
    \item \textbf{A suite of nine tasks}, each derived from pre-existing corpora and targeting distinct aspects of language
    understanding (e.g., linguistic acceptability, sentiment analysis, paraphrase detection).
    \item \textbf{A diagnostic dataset} for an in-depth evaluation of model performance in capturing various linguistic phenomena.
    \item \textbf{A public leaderboard and dashboard} that enable continuous tracking of benchmark performance and provide
    visualization of model results on the diagnostic tasks.
\end{itemize}

Below is a summary table of the key characteristics of the datasets included in the GLUE benchmark:

\input{tab/glue.tex}

In summary, the GLUE benchmark provides a robust foundation for evaluating both standard and domain-adapted NLP models.
Its comprehensive design and the inclusion of diverse linguistic tasks allow for a nuanced analysis of model capabilities.
Following this overview, the FLUE benchmark, which is tailored for the evaluation of models in the financial context,
will be discussed to further complement the assessment of domain-adaptive pre-training strategies.

The Financial Language Understanding Evaluation (FLUE) benchmark is a domain-specific analog to the GLUE benchmark,
tailored specifically for the financial domain \parencite{FLANG2022FLUE}. This benchmark was developed very recently based
on 5 diverse datasets. Its creation was driven by the need to evaluate models capable of effectively processing
financial texts, as standard general-purpose datasets often fail to capture the unique characteristics of financial
lexicon and the specific tasks inherent to this domain.

\input{tab/flue.tex}

FLUE covers 5 distinct financial tasks, which allow for a comprehensive evaluation of model performance across various aspects
of financial language. The statistics presented in Table~\ref{tab:FLUE} demonstrate the scale and diversity of the included datasets.
Moreover, all datasets that comprise FLUE are characterized by low ethical risks and do not contain confidential information regarding
any organization or individual. In addition, explicit consent was obtained from the authors of each dataset prior to their inclusion
in the benchmark, underscoring its legitimacy and ethical soundness.

The emergence of the FLUE benchmark is driven by the necessity to standardize the evaluation of models in the field of financial
language understanding. The financial sector imposes unique requirements for processing textual data, such as high terminological
complexity, market dynamism, and specific tasks (e.g., sentiment analysis of news headlines, information extraction, etc.). These
factors have led to the creation of a heterogeneous set of tasks unified within FLUE, thereby enabling a holistic assessment
of different models. Thus, FLUE serves as an essential tool for researchers, facilitating objective model comparison
and the identification of areas for further improvement in financial NLP approaches.