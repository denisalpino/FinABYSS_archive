\subsubsection{Модели}
Изначально в аналитике финансовых текстов применялись классические рекуррентные \parencite{RNN1986} и сверточные \parencite{CNN1998lecun} сети.
Рекуррентные сети, включая улучшенные варианты LSTM \parencite{Hochreiter1997LSTM}, способны обрабатывать текст как последовательность,
моделируя контекст слова на основе предыдущих состояний. Поворотным этапом стал переход к трансформерам \parencite{vaswani2017attention}.


\textbf{Представления двунаправленного кодировщика трансформера (Bidirectional Transformer Encoder Representations,
BERT)}. Модель BERT \parencite{devlin2019BERT} предложила двунаправленный кодировщик на основе механизмов внимания,
что позволило учитывать контекст слова одновременно слева и справа. BERT задаёт новую парадигму: модель предобучается
на больших корпусах на задаче моделирования маскированного языка (Masked Language Modeling, MLM), а затем тонко
настраивается под конкретные задачи.

Ключевой архитектурной особенностью BERT является полносвязная модель трансформера с механизмом Multi-Head Attention
и скрытыми слоями Feed-Forward (в BERT-base – 12 слоёв по 768 нейронов). При этом BERT базово ограничен длиной входной
последовательности (до 512 токенов), что ставит ограничения на обработку длинных документов. Зато архитектура BERT
оказалась крайне удачной для тонкой настройки: она хорошо зарекомендовала себя в классификации, вопросно-ответных
системах и других задачах при минимальных изменениях слоёв вывода.



\textbf{Финансовый BERT (FinBERT)}. Для финансового домена были разработаны адаптированные версии BERT --- FinBERT.
Так, в 2019 году была разработана модель FinBERTv1 \parencite{Araci2019FinBERT}, дообученный BERT на финансовом датасете для задачи
анализа тональности. Его модель показала улучшение всех метрик. FinBERTv1 использует стандартную архитектуру BERT,
но имеет специфический словарь финансовых текстов — например, термины из отчетов и новостей. Несмотря на размер
обучающей выборки меньший, чем у моделей общего назначения, Успех FinBERTv1 прежде всего обусловлен тем, что
модель «понимает» контекст финансовой терминологии лучше, чем модели общего назначения.

Позже была разработана другая версия FinBERTv2, специально предобучая её на финансовых корпусах (новости,
отчёты компаний, финансовые форумы) и используя мультизадачное обучение \parencite{Liu2020FinBERT}. Это
позволило FinBERTv2 выучить богатые представления, учитывающие специфику финансовой лексики. FinBERTv2
демонстрирует заметно лучшие результаты на задачах финансового сентимента и вопросно-ответных системах.

Еще одна работа — FinBERTv3 \parencite{Yang2020FinBERT, Huang2023FinBERT} --- также адаптировала BERT
под финансы, но с акцентом на интерпретируемость и извлечение информации из аналитических отчётов. Авторы
развивают идею учета контекста: их FinBERTv1 «интегрирует финансовые знания» и лучше «резюмирует
контекстуальную информацию в финансовых текстах». При этом модель значительно превосходит как словарные
подходы, так и классические алгоритмы машинного обучения, а также свёрточные и LSTM-модели на задачах
классификации тональности в экспериментах с разметкой аналитиков. Особенно сильным преимуществом FinBERTv3 \parencite{Yang2020FinBERT}
является обработка предложений, где другие модели ошибочно ставят метку «нейтрально» --- благодаря лучшему
учёту контекста модель выявляет позитивные/негативные оттенки там, где неглубокие методы промахиваются.

Таким образом, все три варианта FinBERT основаны на архитектуре трансформера BERT, но различаются стратегией
обучения: модель FinBERTv1 просто тонко настроена на специализированном датасете, FinBERTv2 --- предобучена
смешанных данных и задачах, а FinBERTv3 --- упор сделан на извлечение финансовой информации, контекстный
анализ и интерпретацию. Во всех случаях финансовая специализация даёт превосходство над «чистой» моделью
BERT в финансовом домене.

\textbf{ModernBERT} \parencite{Warner2024ModernBERT} --- это новейшее поколение моделей, кодировщиков, эта архитектура сохраняет идею BERT,
но включает несколько усовершенствований. Во-первых, ModernBERT обучен на очень большом объёме данных ($\approx$3
триллиона токенов) и изначально поддерживает длину последовательности до 8192 токенов, в то время как исходный
BERT ограничен 512 токенами. Это качественно меняет способности модели — она может захватывать длинный контекст
документа целиком без специальной пост-обработки. Во-вторых, архитектура ModernBERT включает технологические
новшества: так, в ней применены гейтированные слои GeGLU, использованы ротационные позиционные кодировки (RoPE)
для лучшего кодирования положения на больших расстояниях, а также внедрён чередующийся механизм локально-глобального
внимания вместо полного механизма внимания у BERT. Это позволяет эффективнее обрабатывать длинные тексты с точки
зрения вычислительных ресурсов. Кроме того, ModernBERT оптимизирован для скорости и памяти: он спроектирован
с учётом аппаратных особенностей GPU.

По сравнению с BERT и его «каноническими» модификациями, ModernBERT достигает новых рекордов эффективности. Так,
эта модель превосходит BERT на всех стандартных задачах бенчмарка GLUE (см. Раздел \ref{sec:benchmarks}) и значительно
улучшает показатели в задачах извлечения и поиска по длинным контекстам. Её показатели значительно выше конкурентов
в задачах с длинным контекстом. Например, при построении индексов ColBERT ModernBERT имеет метрику 80.2, в то время,
как для BERT она составляет всего 28.1, а для предыдущей модели с аналогичным контекстом --- 69.3. При этом ModernBERT
также обрабатывает короткий контекст быстрее предыдущих лидирующих моделей, сохраняя при этом экономию памяти
\parencite{Warner2024ModernBERT, devlin2019BERT}. Иными словами, ModernBERT представляет собой «новое поколение»
кодировщиков. Он не только эффективен, но и специально заточен под длинные тексты и большие масштабы данных.

Таким образом, ModernBERT — это эволюционная ступень архитектуры кодировщика. Он сохраняет концепцию двунаправленного
трансформера BERT, но адаптирует её к масштабным данным и длинным последовательностям. Благодаря сочетанию новых
архитектурных приёмов и гигантского корпуса обучения ModernBERT значительно расширяет возможности по обработке финансовых
(и любых других) текстов большого объёма по сравнению с оригинальным BERT.

\subsubsection{Техники}
\textbf{Тонкая настройка.} В современных NLP-системах, основанных на трансформерах, предобученная модель служит
обобщённым языковым ядром, однако она не всегда готова давать оптимальные представления эмбеддингов для конкретных
задач. Тонкая настройка представляет собой дополнительную фазу обучения, в ходе которой уже предобученная модель
корректирует свои внутренние веса под узконаправленные цели. Тонкая настройка задействует размеченные данные
именно для целевой задачи (классификация, регрессия и т. д.). В финансовом тематическом анализе тонкая настройка
критична для того, чтобы модель не просто генерировала универсальные представления текста, а акцентировала внимание
на релевантных паттернах, интенсивности финансовых терминов, синтаксических конструкциях и даже количественных
упоминаниях, что обеспечивает большую точность кластеризации и прогнозных модулей.

Одной из наиболее востребованных downstream-задач является оценка семантичесой схожести текстов (Semantic Textual
Similarity, STS) --- определение смыслового сходства двух текстовых фрагментов. В отличие от бинарных задач классификации,
STS формулируется как задача регрессии: модели требуется выдать непрерывную оценку близости, обычно в диапазоне
от 0 до 5 \parencite{Cer2017STSB} или от 0 до 1 \parencite{gao2021simcse}.

STS-задача предъявляет высокие требования к качеству эмбеддингов: они должны сохранять тонкие семантические нюансы
и одновременно быть инвариантны к синтаксическим перестановкам и стилевым вариациям. Так, была предложена архитектура
Sentence-BERT, где два экземпляра BERT-сети обмениваются градиентами через сиамскую конфигурацию, а их выходные векторы
сравниваются по косинусному расстоянию, что позволило значительно ускорить и улучшить оценку STS \parencite{SBERT2019}.

Таким образом, тонкая настройка эмбеддинговой модели на STS играет ключевую роль в построении пайплайна, где комбинируются
модели понижения размерности и кластеризации. Тонкая настройка обеспечивает эмбеддинги, в которых локальные и глобальные
семантические связи выстроены таким образом, чтобы и нелинейное понижение размерности сохраняло релевантные соседства,
и плотностная кластеризация автоматически выделяла тематические объединения без избыточного шума.

\textbf{Доменно-адаптированное предобучения (Domain-Adaptive Pretraining, DAPT).} В современной практике работы с
LLM оказывается недостаточным одноразовое предобучение на обширных общекорпусных данных. DAPT представляет собой
дополнительную фазу предобучения на текстах целевого домена, предшествующую этапу тонкой настройки на определенной
задаче \parencite{gururangan2020DAPT}.

В отличие от классической тонкой настройки, при которой модель лишь корректирует веса в последних слоях под конкретный
датасет и задачу, DAPT продлевает исходную задачу языкового моделирования, позволяя модели «освежить» и углубить своё
представление о лексике, синтаксисе и семантике специфического домена --- например, финансовых публикаций. Такое поэтапное
наращивание преобучения демонстрирует существенные приросты качества на доменных задачах как в условиях дефицита
размеченных данных, так и при их изобилии.

Исходя из расчетов, DAPT в среднем обеспечивает прирост бенчмарков на 4\% в относительной шкале по сравнению с базовой
моделью, которая не была адаптирована под определенный домен. Данная цифра является весьма весомой, учитывая, что
для некоторых специфичных задач прирост может составлять вплоть до 20\% \parencite{gururangan2020DAPT}.

Необходимость доменной адаптации особенно очевидна в финансовом секторе: общие корпуса, такие как BookCorpus \parencite{BookCorpus2015}
или Wikipedia \parencite{WikiText2017}, не отражают терминологию, метафоры и стилистические паттерны пресс-релизов, аналитических отчётов
и биржевых новостей. Множество исследований подтвердило преимущества DAPT в специализированных областях. Так,
разработчики SciBERT продемонстрировали, что предобучение BERT на корпусе научных статей приводит к улучшению
показателей на задачах извлечения научных сущностей и классификации статей по дисциплинам, обеспечивая рост
F1-метрики до 2-3 пунктов по сравнению с базовой моделью \parencite{SciBERT2019}. Аналогично BioBERT, адаптированный
к биомедицинскому тексту, повысил точность извлечения медицинских терминов и отношений на 10-20\% по сравнению
с исходным BERT \parencite{BioBERT2020}.

В финансовом домене доменная адаптация позволяет LLM лучше улавливать нюансы упоминаний акций, процентных ставок
и регуляторных терминов, что вкупе с задачей тематической кластеризации повышает качество эмбеддингов и делает
выходные представления более «разборчивыми» при последующей кластеризации методом HDBSCAN. Таким образом, DAPT
выполняет роль «моста» между общими языковыми знаниями и потребностями предметной области, позволяя добиться
значительного улучшения результатов на доменных задачах.

\textbf{Механизм слияния.} В многомодальных и многокомпонентных системах механизм слияния отвечает за объединение
информации из разных источников или представлений, обеспечивая совместную обработку гетерогенных сигналов.

Первый --- раннее слияние --- предполагает подачу всех признаков (биржевых котировок, технических индикаторов
и текстовых эмбеддингов) сразу в единую модель CNN‑LSTM \parencite{Karpathy_2014_CVPR, dutt2022shared}. Преимущество
метода --- простота реализации и возможность немедленно учить кросс‑модальные зависимости. Однако на практике
раннее слияние подвержен «захлёбыванию» в шуме одной из модальностей и утрачивает гибкость при динамической
оценке вклада каждой из них \parencite{dutt2022shared}.

Второй метод --- позднее слияние --- объединяет предсказания отдельных каналов (каждая модальность обрабатывается
своей CNN-LSTM-ветвью) лишь на завершающем этапе \parencite{Karpathy_2014_CVPR, ortega2019multimodal}.
Такой подход отличается модульностью (простота замены или дообучения отдельного канала), но исключает
извлечение низкоуровневых кросс‑модальных закономерностей и требует обучения всех ветвей по‑отдельности,
что влечет кратное увеличение вычислительных ресурсов \parencite{dutt2022shared}.

Третий, компромиссный механизм --- медленное слияние --- обеспечивает поэтапное слияние каналов
на разных уровнях сети \parencite{feichtenhofer2016convolutional, dutt2022shared}. Ключевые преимущества метода:
баланс между автономной переработкой каждой модальности и возможностью учёта их взаимодействия, сохранение
«чистоты» низкоуровневых признаков и гибкость настройки числа и глубины этапов интеграции \parencite{Karpathy_2014_CVPR}.
Главные недостатки --- сложность выбора оптимального уровня слияния и повышенные вычислительные затраты из‑за
параллельных ветвей на ранних слоях, но тем не менее, меньшие нежели при позднем слиянии.

\subsubsection{Бенчмарки}
\label{sec:benchmarks}
Бенчмарк общего пониания языка (General Language Understanding Evaluation, GLUE) представляет собой набор из девяти
разнообразных задач на понимание естественного языка, предназначенных для оценки и сравнения производительности моделей
в различных задачах обработки естественного языка (Natural Language Processing, NLP) \parencite{wang2018GLUE}.
Предоставляя стандартизированную основу, GLUE облегчает разработку моделей, которые хорошо обобщаются при решении
различных задач, способствуя прогрессу в создании надежных и универсальных систем понимания языка.

Бенчмарк GLUE представляет собой набор ресурсов для обучения, оценки и анализа систем понимания естественного языка.
GLUE состоит из:

\begin{itemize}
    \item \textbf{Набор из девяти задач}, на понимание языка по предложениям или парам предложений, созданный на основе
    существующих наборов данных и отобранный таким образом, чтобы охватить разнообразный диапазон размеров наборов
    данных, жанров текстов и степеней сложности.
    \item \textbf{Диагностический набор данных}, предназначенный для оценки и анализа работы моделей в отношении
    широкого спектра лингвистических явлений, встречающихся в естественном языке;
    \item \textbf{Публичная таблица лидеров} для отслеживания результатов выполнения эталона и приборная панель
    для визуализации работы моделей на диагностическом наборе.
\end{itemize}

Ниже представлена таблица, иллюстрирующая ключевые характеристики датасетов, используемых в GLUE:

\input{tab/glue.tex}

В целом, бенчмарк GLUE представляет собой надежную основу для оценки как стандартных, так и адаптированных к конкретной области моделей NLP.
Его комплексный дизайн и включение различных лингвистических задач позволяют провести тонкий анализ возможностей модели.
После этого обзора будет рассмотрен эталон FLUE, предназначенный для оценки моделей в финансовом контексте,
будет рассмотрен для дальнейшего дополнения оценки доменно-адаптивных стратегий предварительного обучения.

Бенчмарк Financial Language Understanding Evaluation (FLUE) представляет собой аналог бенчмарка GLUE, но специализированный
под домен финансов \parencite{FLANG2022FLUE}. Данный бенчмарк был создан совсем недавно на основе 5 разнообразных датасетов.
Его разработка обусловлена необходимостью оценки моделей, способных эффективно обрабатывать финансовый текст, поскольку
стандартные универсальные наборы данных зачастую не отражают специфические особенности финансовой лексики и задач,
присущих данной области.

\input{tab/flue.tex}

FLUE охватывает 5 различных финансовых задач, что позволяет проводить комплексную оценку качества моделей на разнообразных
аспектах финансового языка. Статистика, представленная в Таблице \ref{tab:FLUE}, демонстрирует величину и разнообразие датасетов.
При этом все датасеты, входящие в состав FLUE, характеризуются низким уровнем этических рисков и не содержат конфиденциальной
информации ни о каких организациях или отдельных лицах. Дополнительно, для включения каждого датасета в бенчмарк был получен
соответствующий запрос на согласие авторов, что подчеркивает его легитимность и корректность с точки зрения этики.

Возникновение бенчмарка FLUE продиктовано потребностью стандартизировать оценку моделей в области финансового понимания языка.
Финансовая сфера предъявляет особые требования к обработке текстовых данных: высокая терминологическая сложность, динамичность
рынка, специфические задачи (например, анализ тональности новостных заголовков, извлечение информации и другие). Именно эти факторы
способствуют формированию разнородного набора задач, объединённых в FLUE, что позволяет всесторонне оценивать эффективность различных
моделей. Таким образом, FLUE служит важным инструментом для исследователей, способствуя объективному сравнению моделей и выявлению
областей для дальнейшего совершенствования подходов в финансовом NLP.