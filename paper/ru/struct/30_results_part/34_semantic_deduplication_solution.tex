\subsubsection{Математическая постановка}
\label{sec:semantic_deduplication}
\sloppy  % Helps to alleviate overfull hbox warnings

В рамках исследования был разработан новый подход к дедубликации, основанный на анализе семантического
одержимого объектов. Хотя в настоящей работе сущностью выступает текст, метод легко обобщается на любые
объекты, допускающие векторное представление в семантическом пространстве.

Каждая статья представляется в виде последовательности эмбеддингов:

\begin{equation}
    x_i \subset \mathbb{R}^{t\times d}
\end{equation}

где $t$ --- число токенов, а $d$ --- размерность семантического векторного пространства.
Для последующего анализа вместо набора эмбеддингов используется их выпуклая оболочка,
обозначаемая как $\operatorname{CH}(x_i)$ или, сокращенно, $\operatorname{CH}_i$. В качестве
меры уникальности текста применяется объем этой оболочки, $\mathrm{vol}(\operatorname{CH}_i)$.

\textbf{Пересечение выпуклых оболочек}. При прямом вычитании пересечений между
$\operatorname{CH}_i$ и оболочками других текстов может возникнуть проблема
множественного учета. Чтобы устранить кратное вычитание, применяется метод
включений–исключений.

Обозначим множество всех статей, кроме $i$, как

\begin{equation}
    \mathbb{I} = \{1, \ldots, N\} \setminus \{i\}.
\end{equation}

Пересечение $\operatorname{CH}_i$ с оболочками статей, индексированных подмножествами $\mathbb{J} \subseteq \mathbb{I}$, задается выражением:

\begin{equation}
    \mathrm{vol}\Big(\operatorname{CH}_i \cap \bigcap_{j \in \mathbb{J}}\operatorname{CH}_j\Big).
\end{equation}

Тогда объем пересечения $\operatorname{CH}_i$ с объединением оболочек остальных статей вычисляется по формуле:

\begin{equation}\label{eq:inclusion-exclusion_substituting}
    \mathrm{vol}\left(\operatorname{CH}_i \cap \bigcup_{j \in \mathbb{I}}\operatorname{CH}_j\right)
    =
    \sum_{k=1}^{N-1} (-1)^{k-1} \sum_{\substack{\mathbb{J} \subseteq \mathbb{I} \\ |\mathbb{J}| = k}}
    \mathrm{vol}\left(\operatorname{CH}_i \cap \bigcap_{j \in \mathbb{J}}\operatorname{CH}_j\right).
\end{equation}

Уникальность статьи $i$ определяется как доля объема её выпуклой оболочки, не задействованная в пересечениях с оболочками других статей:

\begin{equation}
    \mu_i = \frac{\mathrm{vol}\Big( \operatorname{CH}_i \setminus \bigcup_{j \in \mathbb{I}} \operatorname{CH}_j\Big)}{\mathrm{vol}\Big( \operatorname{CH}_i\Big)}.
\end{equation}

Преобразуя это определение с учетом разбиения $\operatorname{CH}_i$ на область пересечения и её дополнения, получаем:

\begin{equation}
    \mu_i = 1 - \frac{\mathrm{vol}\left(\operatorname{CH}_i \cap \bigcup_{j \in \mathbb{I}}\operatorname{CH}_j\right)}{\mathrm{vol}\left(\operatorname{CH}_i\right)}.
\end{equation}

Подставляя выражение по принципу включений–исключений, окончательная Формула \ref{eq:inclusion-exclusion_substituting} имеет вид:

\begin{equation}\label{eq:uniqueness}
    \mu_i = 1 - \frac{1}{\mathrm{vol}\big(\operatorname{CH}_i\big)}
    \sum_{k=1}^{N-1} (-1)^{k-1} \sum_{\substack{\mathbb{J} \subseteq \mathbb{I} \\ |\mathbb{J}| = k}}
    \mathrm{vol}\Big(\operatorname{CH}_i \cap \bigcap_{j \in \mathbb{J}}\operatorname{CH}_j\Big).
\end{equation}

Значение $\mu_i \in [0, 1]$ характеризует уникальность текста: $\mu_i = 1$ означает отсутствие пересечений с другими текстами
(полная уникальность), а $\mu_i = 0$ указывает на то, что семантический объём текста полностью занят пересечениями с оболочками других текстов.

\subsubsection{Преимущества и недостатки}
Предлагаемый метод основан на теоретически обоснованном представлении текста: каждая
статья трактуется как выпуклая оболочка эмбеддингов токенов, что позволяет четко определить
семантическое содержимое объекта, применить принцип включения–исключения, унаследованный из
теории множеств, для корректного вычисления объёма пересечений, а также нормировать результат
так, чтобы итоговая мера уникальности находилась в интервале $[0,1]$.

У данного подхода помимо теоретической стройности есть целый ряд дополнительных преимуществ:
\begin{itemize}
    \item Применение эмбеддингов для каждого токена позволяет уловить тонкие различия
    в семантическом содержании, а агрегирование посредством выпуклой оболочки обеспечивает
    обобщённое представление о содержании текста. Это даёт возможность сравнивать тексты
    различной длины и тематики в едином векторном пространстве.
    \item Нормировка метрики, выраженной числом в интервале $[0,1]$, упрощает интерпретацию.
\end{itemize}

С другой стороны, у метода есть и ряд основательных недостатков:

\begin{itemize}
    \item Выпуклая оболочка в высокоразмерном пространстве (например, 768-мерном) может существенно «растянуться».
    Это приводит к неинформативности получаемых объёмов, а геометрия оболочек может не отражать сложные распределения эмбеддингов.
    \item Эмбеддинги имеют сложную, зачастую нелинейную структуру. Выпуклая оболочка, являясь минимально необходимым выпуклым множеством,
    может включать экстремальные точки, что приводит к переоценке занимаемого пространства и, как следствие, к искажению оценок.
    \item Из-за того, что эмбеддинги могут содержать случайные шумовые компоненты или артефакты, выпуклая оболочка может быть чувствительна к выбросам.
    Это приводит к тому, что небольшие неточности в эмбеддингах могут непропорционально увеличить объём выпуклой оболочки, и, соответственно, исказить оценку уникальности.
    \item Построение выпуклой оболочки и вычисление объёмов в высокоразмерном пространстве является ресурсозатратной задачей. Применение принципа включения–исключения для
    корректного вычисления пересечений между оболочками текстов усложняет расчёты, особенно при большом количестве документов.
\end{itemize}

Все перечисленные проблемы могут критичным образом сказаться на использования данного метода на практике, однако некоторых проблем можно избежать инженерным образом.

Проблему чувствительности к шуму (пункт 3) можно частично нивелировать использованием токена [CLS] в качестве
центроида выпуклой оболочки. Введение коэффициента $\delta$ для нормирования «вогнутости» оболочки в направлении
эмбеддинга [CLS] позволяет снижать влияние шумовых компонентов.

Проблему вычислительных затрат (пункт 4) можно решать различными способами:

\begin{itemize}
    \item Регулирование числа включающих–исключающих пар (гиперпараметр $N$ в суммировании)
    позволяет получить приближённую оценку уникальности с уменьшением вычислительной нагрузки.
    \item Применение алгоритмов, таких как UMAP, t-SNE, PCA и других, может привести исходное пространство
    к более низкой размерности, что значительно сократит затраты на вычисления. При этом необходимо учитывать возможную потерю точности.
    \item Аппроксимация объёма методом Монте–Карло позволяет получить оценку при уменьшении вычислительных ресурсов.
\end{itemize}

Метод представления семантической уникальности текста через выпуклые оболочки эмбеддингов обладает рядом
теоретических преимуществ (сильная нормировка, применимость принципа включения–исключения и единообразная
интерпретация результата). Однако практическое применение требует решения проблем, связанных с высокой размерностью,
нелинейностью распределения эмбеддингов и существенными вычислительными затратами. Дальнейшие исследования
могут быть направлены на разработку более устойчивых и эффективных методов оценки уникальности с учетом
указанных ограничений.