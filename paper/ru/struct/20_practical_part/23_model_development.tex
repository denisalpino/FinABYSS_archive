\subsubsection{Извлечение векторных представлений}
После предобработки корпуса текстов и удаления фонового шума, для ускорения последующих этапов тематического моделирования,
включая обучение моделей понижения размерности и кластеризации, были извлечены их эмбеддинги.

Базовая модель ModernBERT не оптимальна для этой задачи, поскольку её векторные представления оказываются чрезмерно разреженными.
Высокая степень разреженности эмбеддингов ухудшает качество кластеризации, в частности методов, основанных на оценке плотностей
(например, DBSCAN). Несмотря на более высокую устойчивость HDBSCAN к различиям плотностей, разреженность всё равно негативно
сказывается на результатах кластерного разбиения.

Чтобы устранить указанную проблему, обычно применяют тонкую настройку модели под задачу STS. При этом модель получает на вход пару
текстов и возвращает оценку их сходства \parencite{MTEB2023}, чаще всего вычисляемую через косинусное расстояние (Формула \ref{eq:cos_dist}).

\begin{equation}\label{eq:cos_dist}
    D_{\cos}(\mathbf{u}, \mathbf{v})
    = 1 - \frac{\mathbf{u} \cdot \mathbf{v}}
                 {\|\mathbf{u}\|_2 \,\|\mathbf{v}\|_2}.
\end{equation}

Именно STS‑модели демонстрируют наиболее плотные и информативные эмбеддинги.

Так, для базовой модели ModernBERT было рассмотрено 2 тонко настроенные модели --- 'modernbert-embed' от Nomic AI \parencite{nomic2024}
и `gte-modernbert-base` от Alibaba \parencite{MGTE2023,MGTE2024}. Для оценки данных двух моделей был использован бенчмарк Massive
Text Embedding Benchmark (MTEB) \parencite{MTEB2023}, охватывающий восемь классов задач и 58 датасетов. По результатам:

\begin{itemize}
    \item В задаче кластеризации на 12 датасетах 'gte-modernbert-base' превзошла 'modernbert-embed' на 1.5 процентных пункта (44.98\% против 44.47\%).
    \item В задаче STS на 10 датасетах их результаты близки (81.78\% и 81.57\% соответственно).
    \item В среднем по всем восьми задачам на 56 датасетах 'gte-modernbert-base' опережает 'modernbert-embed' на 1.76 процентных
    пункта (64.38\% против 62.62\%).
\end{itemize}

В связи с этим для извлечения эмбеддингов была выбрана модель gte-modernbert-base из библиотеки 'sentence\_transformers' \parencite{SBERT2019}.
При получении эмбеддингов для всего документа вместо токена [CLS] применялась более продвинутая техника --- Mean Pooling, которая заключается
в усреднении значений эмбеддингов по всем токенам последовательности (Формула \ref{eq:mean_pooling}).

\begin{equation}\label{eq:mean_pooling}
    \mathbf{h_{mean}}=\frac{1}{T}\sum^T_{t=1}\mathbf{h}_{t}.
\end{equation}

где $T$ — длина токенизированной последовательности, а $\mathbf{h}_t$ — эмбеддинг $t$-го токена.

Для оптимизации скорости обучения моделей понижения размерности и кластеризации,
эмбеддинги предварительно приводились к единичной $L_2$-норме (Формула \ref{eq:l2_norm}):

\begin{equation}\label{eq:l2_norm}
    \widehat{\mathbf{u}}
    = \frac{\mathbf{u}}{\|\mathbf{u}\|_2},
    \qquad
    \|\mathbf{u}\|_2 = \sqrt{\sum_{i=1}^{n} u_i^2}.
\end{equation}

Подобная предобработка позволяет пользоваться GPU-оптимизированной евклидовой метрикой расстояния (Формула \ref{eq:euclidean}) при обучении модели DR,
не повторяя при этом на каждой итерации оптимизации гиперпараметров вычисление $L_2$-нормы, которое происходит при вычислении метрики косинусного расстояния.

\begin{equation}\label{eq:euclidean}
    D_{2}(\mathbf{u}, \mathbf{v})
    = \|\mathbf{u} - \mathbf{v}\|_2
    = \sqrt{\sum_{i=1}^{n} \bigl(u_i - v_i\bigr)^2}.
\end{equation}

Таким образом, после вычисления $L_2$-нормы и евклидова расстояния, мы, на самом деле, в некотором смысле можно считать, что мы работаем с косинусным расстоянием,
так как приведенная мера становится монотонно связанной с косинусным и отражает тот же порядок близости точек (Формула \ref{eq:euclidean_after_l2_norm}),
однако все вычисления ускоряются за счет GPU-оптимизации.

\begin{equation}\label{eq:euclidean_after_l2_norm}
    D_{2}\bigl(\widehat{\mathbf{u}}, \widehat{\mathbf{v}}\bigr)
    = \bigl\|\widehat{\mathbf{u}} - \widehat{\mathbf{v}}\bigr\|_2
    = \sqrt{2\,\bigl(1 - \widehat{\mathbf{u}}\!\cdot\!\widehat{\mathbf{v}}\bigr)}.
\end{equation}

Для построения и обучения алгоритмов понижения размерности и кластеризации была сформирована
тренировочная выборка из 200 000 эмбеддингов и сопутствующих метаданных, что составляет приблизительно
16\% от всего корпуса. Такой объем обучающей подвыборки был выбран, исходя из доступных вычислительных
ресурсов. Так, 200 000 эмбеддингов использовались для подбора оптимальных гиперпараметров,
а остальные 1 050 000 эмбеддингов были зарезервированы для этапов валидации и инференса.
При этом, перед инференсом конвейер моделей понижения размерности и кластеризации были обучены
на всем корпусе с линейным увеличением значений гиперпараметров для алгоритма HDBSCAN, выбор которого
обуславливается в \hyperref[sec:drc]{Секции 2.3.2}.

Кроме того, при извлечении эмбеддингов была задействована смешанная точность (float16) и ускоренный
механизм внимания FlashAttention \parencite{flash2022attention}, что существенно сократило требования
к вычислительным ресурсам и время обработки.

\subsubsection{Модели понижения размерности и кластеризации}
\label{sec:drc}
Итак, после формирования выборки эмбеддингов мы приступили к экспериментам с моделями DR и кластеризации,
проводя их совместную оптимизацию. Такой подход обусловлен многокритериальным характером задачи: требуется
не только сохранить структурные (глобальные) и локальные взаимосвязи из исходного 768-мерного пространства,
но и обеспечить разделимость («кластерность») эмбеддингов в низкоразмерной проекции, пригодной для визуализации на плоскости.

В качестве ключевых условий было обозначено:

\begin{enumerate}
    \item Сохранение кластерной структуры. Эмбеддинги после понижения размерности должны оставаться разделимыми,
    то есть сохранять группировку по смыслу и тематике.
    \item Готовность к двумерной визуализации. Итоговое пространство должно быть пригодно для наглядного
    отображения на плоскости без потери интерпретируемости.
\end{enumerate}

Для одновременного поиска оптимальных гиперпараметров выбора алгоритмов понижения размерности
и методов кластеризации использовался единый мета‑процесс.

В качестве критерия оптимальности был принят индекс DBCV, поскольку он не предполагает заранее заданной
формы кластеров (в отличие от индексов, ориентированных на сферические или эллипсоидальные структуры)
и эффективно оценивает плотностные методы кластеризации.

В качестве основы был выбран алгоритм HDBSCAN \parencite{HDBSCAN2013}, отвечающий двум важным требованиям:

\begin{itemize}
    \item Отсутствие предположений о форме кластеров. В отличие от K‑Means HDBSCAN не предполагает,
    что кластеры являются гауссовыми сферами, что критически для представления тем.
    \item Иерархическая и плотностная природа. Позволяет выявлять как крупные тематические группы,
    так и мелкие, высококонцентрированные ниши.
\end{itemize}

Кроме того, благодаря реализации на GPU, HDBSCAN показал высокую скорость обработки
как больших выборок, так и высокоразмерных данных.

В контексте алгоритма HDBSCAN для оптимизации рассматривается несколько ключевых гиперпараметров.
Минимальное число соседей --- количество точек в окрестности, необходимое для признания точки «ядром» кластера.
Минимальный размер кластера --- число наблюдений, определяющее порог для формирования кластера,
что позволяет учитывать редкие, узкотематические группы.

 В ходе пилотных экспериментов было обнаружено, что одновременно существуют плотные области («горячие темы»)
 и редкие, но значимые по содержанию кластеры. При малом минимальный размер кластера удаётся захватывать
 редкие темы, однако возрастает число микрокластеров, разделение между которыми семантически не всегда оправдано.

 Для смягчения этой проблемы рассматривалась техника объединения кластеров по порогу $\epsilon$ \parencite{HDBSCAN2020cluster_selection_epsilon}, позволяющая
 консолидировать соседние микрокластеры в высококонцентрированных регионах. Однако такой подход усложняет
 инференс модели: при появлении новых наблюдений $\epsilon$‑объединение не может быть учтено, что вынуждает
 полностью переобучать модель.

 Так как исследование ставит своим приоритетом практичность, было принято решение отказаться от использования
 $\epsilon$, но использовать меньшие значения минимального размера кластера, принимая некоторую фрагментацию,
 но сохраняя возможность последующей интерпретации и агломерации кластеров на более высоком уровне иерархии.

 Ещё один гиперпараметр --- метод выделения кластеров --- определяет, будут ли они формироваться на основе «избытка массы» или «листьев дерева».
 Практика показала, что второй вариант даёт более мелкозернистые и однородные группы, что и было избрано для окончательной настройки.

 Таким образом, на финальной стадии оптимизации HDBSCAN оставлены лишь два подлежащих настройке параметра: минимальное
 число соседей и минимальный размер кластера.

 Также стоит отметить, что в архитектуре, описанной в \hyperref[sec:architecture]{Разделе 3.3},
 планируется фиксированное количество кластеров для статичного числа экспертов,
 поэтому в качестве базовой реализации используется именно HDBSCAN из cuML,
 а не его адаптивная версия \parencite{HDBSCAN2022adaptive}.

 Переходя к алгоритмам понижения размерности, для предварительного отбора рассматривались
 t-SNE, PCA, UMAP, TriMap и PaCMAP, причём ключевыми критериями были точность, возможность
 балансировать отображения локальных и глобальных отношений и скорость обучения. Исходя из опыта
 других исследователей в качестве базового алгоритма для сравнения был выбран UMAP \parencite{BERTopic2022}.

Так, исключён из-за недостаточной производительности на больших объёмах данных. PCA рассматривался как
быстрое глобальное приближение, но для конечного понижения размерности не обеспечивает локальную точность.
Тем не менее, был проведён эксперимент с предварительным применением PCA перед основным алгоритмом,
направленный на ускорение вычислений. Эксперимент показал, что пайплайн PCA + UMAP уступает по индексу
DBCV «чистому» UMAP примерно на 37\%, что сделало применение PCA неоправданным.

Дальнейшее сравнение UMAP, TriMap и PaCMAP продемонстрировало схожую точность при проекции в промежуточные размерности,
однако для окончательной двумерной визуализации PaCMAP оказался предпочтительнее за счёт более равномерного распределения
кластеров на плоскости. Тем не менее, поскольку для кластеризации на промежуточном этапе была важна возможность GPU-ускоренной
оптимизации и проверенная устойчивость, выбор пал на UMAP реализованном в cuML \parencite{cuml2020machine}.

\subsubsection{Оптимизация гиперпараметров}

После этапа инициализации моделей последовала комплексная оптимизация гиперпараметров, затрагивающая
семь ключевых параметров: пять для UMAP (число соседей 'n\_neighbors', размер выходного пространства 'n\_components',
параметры 'min\_dist' и 'spread', коэффициент 'negative\_sample\_rate') и два для HDBSCAN ('min\_cluster\_size' и 'min\_samples').
В качестве фреймворков для поиска оптимума были апробированы Ray Tune \parencite{raytune2018liaw} и Optuna \parencite{optuna2019akiba},
причём акцент был сделан на ресурсно-ориентированном подходе, где «ресурсом» выступал размер обучающей подвыборки.
Экспериментально были выбрано 5 этапов обучения с долями от 1.5\% до 10\% общего корпуса, что позволило
оценить масштабируемость методов и устойчивость полученных гиперпараметров.

В контексте Ray Tune применялась схема Байесовской оптимизации с HyperBand (BOHB)
\parencite{TPEandBO2011bergstra, BOHB2018falkner, hyperband2018li, HPOoverview2015shahriari},
однако она не обеспечила должного соотношения скорости и качества при работе с UMAP/HDBSCAN.
Переключившись на Optuna, были реализованы три разновидности «прунеров» --- механизмы
для досрочного отсечения бесперспективных комбинаций при наращивании ресурса.
Первый, 'AdaptiveStablePercentilePruner', удалял худшие по заранее заданному перцентилю
в каждом шаге ресурса, второй, 'CustomPatientPruner', минимизировал риск преждевременного исключения,
отсекая комбинации после $n$ последовательных неудачных шагов с приростом менее $\delta$, а третий,
'NormalPruner', использовал Z-статистику (Формула \ref{eq:z_score}) и заданный перцентиля стандартного
нормального распределения ($\mathcal{N}(0, 1)$).

\begin{equation}\label{eq:z_score}
    z\text{-score}=\frac{m - \mathbb{E}[M]}{\mathbb{V}[M]},
\end{equation}

где $m$ --- текущая метрика и $M$ --- распределение метрик на данном этапе.

Метрикой оценки в ходе итераций стал взвешенный кумулятивный DBCV-индекс (WCDBCV):

\begin{equation}
    WCDBCV_j=\frac{1}{j\sum_{i=1}^jp_i} \sum_{i=1}^j p_i \cdot DBCV_i,
\end{equation}

где $j$ — текущий этап ресурса, а $p_i$ — доля выборки, использованная на $i$-м шаге.

Из всех сочетаний наилучшую эффективность продемонстрировал алгоритм Древовидной оценка Парсена (TPE)
\parencite{TPE2023watanabe, TPEandBO2011bergstra, HPOoverview2015shahriari} в связке с 'AdaptiveStablePercentilePruner',
однако прирост качества оказался недостаточно существенным относительно возросших вычислительных затрат.
В связи с этим последующий поиск гиперпараметров был выполнен без использования прунинга.
В качестве глобальной стратегии применялся классический TPE с 400 испытаниями, из которых половина
выполнялась в «разминочном» режиме случайных конфигураций. Для локальной донастройки возникших
перспективных областей пространства гиперпараметров использовалась адаптивная эволюционная стратегия
(CMAES) \parencite{restartCMAES2005auger, CMAES2024nomura} с механизмом повторного старта IPOP, адаптивной
скоростью обучения и предварительным «тёплым запуском» на базе 15 лучших комбинаций, найденных
на этапе глобальной оптимизации \parencite{CMAES2024nomura, lrCMAES2023nomura, restartCMAES2005auger, warmCMAES2021nomura}.
Бюджет локальной фазы составил 250 испытаний, что обеспечило глубокий поиск вокруг уже высокоэффективных конфигураций.

В результате такого двухуровневого подхода --- сначала широкого исследования методом TPE, затем углублённого
локального поиска CMAES --- удалось сбалансировать широту и глубину оптимизации, повысив устойчивость конвейера моделей.