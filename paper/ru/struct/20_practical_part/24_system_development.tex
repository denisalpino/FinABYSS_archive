После завершения этапа семантической кластеризации и построения тематических групп,
необходимо обработать сформированные кластеры на лингвистическом уровне. В тематическом
моделировании качество представлений тем является ключевым для интерпретации тем,
передачи результатов и понимания закономерностей. Так, необходимо убедиться,
что каждое множество документов действительно характеризуется уникальным набором
ключевых терминов и не содержит артефактов неправильной сегментации. Данная валидация
выходит за пределы чисто метрических оценок и требует эмпирического, «человеческого»
взгляда на то, какие слова действительно определяют содержание темы. В контексте
неразмеченного корпуса такая проверка особенно важна: без исходных меток о тематике
единственным источником информации о семантической однородности кластеров остаётся
текст самой публикации. Более того, существует необходимость в присвоении названий
каждому из кластеров, поэтому крайне важно разработать пайплайн репрезентации тем.

С другой стороны, результатом кластеризации являются метки нижнего уровня, а в контексте
системы подразумевается иерархический подход к репрезентации тем, что ведет к необходимости
извлечения из текущей модели дополнительной информации и ее пост-обработки.

Для упрощения и формализации процесса в FinABYSS был реализован специализированный пайплайн
репрезентации тем, основанный на следующих инструментах:





\begin{itemize}
    \item BERTopic --- для удобства и упрощения процесса обработки лингвистических признаков,
    визуализации взвешенных частот слов, а также построения иерархии.
    \item OpenAI API --- для автоматизации разметки полученных кластеров на основе их лингвистических признаков.
    \item DataMapPlot для упрощения работы с графическим интерфейсом и создания интерактивной семантической карты.
    \item Собственная реализация:
    \begin{itemize}
        \item над BERTopic --- для реализации возможности выбора определенного количества иерархических тематических уровней.
        \item между BERTopic и OpenAI --- для реализации возможности присвоения тематических названий на основе
        лингвистических признаков на более высоких иерархических уровнях, нежели только на самом нижнем.
        \item между BERTopic и DataMapPlot — для тонкой настройки как статической, так и интерактивной семантической карты.
    \end{itemize}
\end{itemize}

Таким образом, первым этапов в пайплайне стала векторизация текстов, то есть приведение их к матричному виду.
С этой целью использовался 'CountVectorizer' из библиотеки 'sklearn', который был настроен настроенная
на извлечение униграмм и биграмм. Это полезно для более точной репрезентации тем, так как ведет к рассмотрению
таких терминов как “central bank”, “monetary policy” и “New York” и совместно, а не отдельно по словам..


Отныне, слова в контексте документов такие уни- и биграммы будут называться <<терминами>> для предотвращения
путаницы, так как, на самом деле, в контексте разработанной системы рассматриваются не слова, а $n$-граммы,
то есть короткие последовательности из $n$ слов.

Также на основе свободного англоязычного словаря стоп-слов \parencite{nothman2018stop} был настроен этап
фильтрации артиклей, предлогов, местоимений, союзов и других частей речи, мешающих отбору по-настоящему
репрезентативных терминов.

Наконец была настроена минимальная частота термина в документе для его включения в матрицу.
Несложно представить ситуацию, когда определенный термин встречается во всех документах только
один раз. Маловероятно, что данный термин отражает определенную тематику. С другой стороны,
в корпусе собрано более миллиона статей и, если не установить минимальную частоту термина,
матрица станет огромной и непрактичной в использовании. Поэтому термины, появляющиеся менее
15 раз во всём корпусе, отсекаются как статистически нерепрезентативные.

Наконец, система подразумевает функционирование в режиме реального времени, поэтому принципиально
важно уметь инкрементально обновлять матрицу терминов, для этого была использована онлайн версия
'CountVectorizer' --- 'OnlineCountVectorizer'.

После формирования матрицы терминов, необходимо сгруппировать их по тематическим кластерам
и определить релевантность терминов для каждой тематической группы, чтобы в дальнейшем,
на основе наиболее репрезентативных терминов сгенерировать названия кластеров.

Частота термина, обратная частота документа (Term Frequency Inverse Document Frequency, TF-IDF)
и Best Match (BM-25) являются аддитивными функциями релевантности. Они используются в большинстве
поисковых систем как основные метрики релевантности. Обе метрики показывают релевантность документа.
Чем выше значение метрик, тем более релевантен документ. При этом, важно отметить, что само значение
метрик не имеет какой-либо значимой интерпретации, кроме как относительная разность релевантности
документов или терминов друг с другом.

В контексте поисковых систем метрика отражает релевантность документа или термина поисковому запросу,
но в текущей системе используется модифицированная TF-IDF метрика, которая выражает релевантность
термина, не по отношению к запросу, а по отношению к теме.

Такая метрика, называется c-TF-IDF \parencite{BERTopic2022}. Лучше всего ее можно объяснить, как формулу TF-IDF принятую для всей
темы, в целом. То есть все документы внутри темы не рассматриваются по-отдельности, а объединяются в один
большой документ, на основе которого происходит подсчет. Так, c-TF-IDF учитывает то, что отличает документы
в одном тематическом кластере от документов в другом.

Таким образом, мы сначала извлекаем частоту термина $x$ в тематическом кластере $c$, которому и принадлежит
термин $x$. В результате получается представление $tf$ на основе классов. А для того, чтобы учесть различия
в размере тематических кластеров, по полученным частотам вычисляется $L_1$-норма.

Затем, вычисляется логарифм суммы единицы и частного от деления среднего количества слов в каждом
из тематических кластеров на частоту слова $x$ во всех кластерах. Так, получается представление $idf$,
которое помогает определить насколько данное слово $x$ редко встречается среди всех остальных классов.
Единица нужна для того, чтобы логарифм всегда был положительным.

Наконец, как и в случае с обычным TF-IDF, полученные репрезентации $tf$ и $idf$ перемножаются
(Формула \ref{eq:c_tf_idf}):

\begin{equation}\label{eq:c_tf_idf}
    w_{x, c} =
    ||tf_{x, c}||_1
    \times
    \log\Bigg(
        1 + \cfrac{
            \frac{1}{||\mathbb{C}||}\sum_{i \in \mathbb{C}}||\mathbb{X}_i||
        }{
            \sum_{i \in \mathbb{C}}x
        }
    \Bigg),
\end{equation}

где $w_{x, c}$ --- релевантность терма $x$ тематическому кластеру $c$,
$\mathbb{X_i}$ --- множество всех термов в тематическом кластере $i$.

Тем не менее, вместо классической формулы c-TF-IDF, было принято решение использовать
ее модифицированный аналог. Некоторые слова или термины встречаются слишком часто
в каждой из тем, однако не считаются типичными стоп-словами для исключения из текста.
Чтобы сгладить самые частые термины в теме, в системе применяется нормализация частот
терминов, то есть извлекается квадратный корень из $tf$, после применения схемы взвешивания.

С другой стороны, несмотря на то, что собранный финансовый корпус велик, он может
не вполне исчерпывающе отражать все лексическое изобилие генеральной совокупности.
Поэтому, для более стабильного результата, к $idf$ было применено преобразование BM-25.
Итоговая формула релевантности выглядит следующим образом:

\begin{equation}
    w_{x, c} =
    \sqrt{||tf_{x, c}||_1}
    \times
    \log\Bigg(
        1 + \cfrac{
            \frac{1}{||\mathbb{C}||}\sum_{i \in \mathbb{C}}||\mathbb{X}_i|| - \sum_{i \in \mathbb{C}}x + 0.5
        }{
            \sum_{i \in \mathbb{C}}x + 0.5
        }
    \Bigg),
\end{equation}

Коэффициенты $0.5$ основаны на подгонке под теоретически более чистую форму.
Такая форма дает меньший вес слишком часто встречающимся терминам.

В итоге, мы получаем для каждого из тематических кластеров мешки слов, которые
достаточно репрезентативно отражают лексическое богатство каждой темы. Тем не менее,
в мешках слов все еще могут присутствовать некоторые не совсем репрезентативные слова
или же мешок может быть излишне однородным. Данные проблемы могут негативно сказаться
на процессе генерации названий тем, поэтому при разработке системы был предусмотрен
более сложный пайплайн, который добавляет еще 2 шага перед финальной репрезентацией.

Первый шаг заключается в семантическом сравнении наиболее репрезентативных слов,
с наиболее репрезентативными документами. Сначала из мешка слов извлекается 30 наиболее
репрезентативных терминов, а из самого тематического кластера --- 5 наиболее репрезентативных
документов. Затем, мы собираем уже извлеченные эмбеддинги по соответствующим документам,
и при помощи той же эмбеддинговой модели —-- 'gte-modetnbert-base' --- извлекаем эмбеддинги
из выбранных терминов. После чего, мы сравниваем эмбеддинги терминов-кандидатов с наиболее
репрезентативными документами и ранжируем их по полученному значению метрики косинусного расстояния.

На втором шаге мы применяем алгоритм Максимальной Предельной Релевантности (Maximaxl Marginal Relevance,
MMR) --- это техника для реферирования по запросу, которая максимизирует похожесть фрагментов ответа
на запрос и минимизирует схожесть с уже выбранными в ответ фрагментами. Аналогично TF-IDF, в контексте
текущей системы рассматривается не запрос, а термин. То есть мы максимизируем внутреннее разнообразие
наиболее репрезентативных теме терминов. MMR вычисляется по следующей формуле:


\begin{equation}\label{eq:mmr}
    MMR =
    \argmax_{
        t_i \in \mathbb{R} \setminus \mathbb{S}
    }[
        \lambda(sim_1(t_i,\mathbb{C})) - (1 - \lambda)\max_{t_j \in \mathbb{S}}(sim_2(t_i, t_j))
    ],
\end{equation}

где $\mathbb{T}$ --- тема, для которой рассчитывается $MMR$, $\mathbb{R}$ --- множество всех терминов $t_i$ --- рассматриваемый термин,
а $\mathbb{S}$ — множество уже выбранных терминов. Получается, что мы ищем новый термин из $mathbb{R} \setminus \mathbb{S}$ так,
чтобы он был максимально похож на тему, но минимально на на уже присутствующие в мешке слов термины. В Формуле \ref{eq:mmr}
коэффициент $\lambda$ является гиперпараметром и балансирует похожесть терминов на тему с непохожестью терминов друг с другом.
Чем меньше $\lambda$, тем термины менее похожи друг на друга, а чем больше, тем похожее термины на тему.

Так, у нас получается максимально репрезентативный набор терминов, относительно которых мы уже можем генерировать названия тем.
Непосредственно для генерации тем в системе предусмотрено использование text2text моделей. Конкретно в текущей реализации
использовалась модель GPT-4o, для которой был задан соответствующий промпт с инструкциями.
Модель использовалась с помощью OpenAI API. А все сгенерированные метки позднее были провалидированы вручную.

Наконец, заключительным этапом является визуализация семантической карты. Визуализация была выполнена
в двух форматах: в статичном --- для работы и презентации, и в интерактивном --- для функционирования системы.
Визуализация осуществлялась при помощи Python-библиотеки DataMapPlot, а также кастомных дополнений на HTML,
CSS и JavaScript. Также на системном уровне были разработаны функциональности для поиска по текстам,
построения облака слов, фильтрации по источникам, активам, дате публикации и другим количественным признакам.