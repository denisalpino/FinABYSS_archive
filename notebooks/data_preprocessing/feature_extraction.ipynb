{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import os\n",
    "os.chdir('/home/denisalpino/dev/FinABYSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda device is available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"{} device is available\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure embedding model based on fine-tuned ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 09:38:03.956018: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747809483.983334  712855 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747809483.989324  712855 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747809484.009231  712855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747809484.009265  712855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747809484.009268  712855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747809484.009271  712855 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-21 09:38:04.015894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"Alibaba-NLP/gte-modernbert-base\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-modernbert-base\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_embed(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n",
    "            inputs = tokenizer(\n",
    "                texts[i:i+batch_size],\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=8192,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Mean pooling\n",
    "            attn_mask = inputs.attention_mask.unsqueeze(-1)\n",
    "            pooled = torch.sum(outputs.last_hidden_state * attn_mask, dim=1) / torch.clamp(attn_mask.sum(dim=1), min=1e-9)\n",
    "            # Mixed Precision\n",
    "            embeddings.append(pooled.half().cpu())\n",
    "            # Cache cleaning\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    result = torch.cat(embeddings).numpy().reshape(len(texts), 768)\n",
    "    torch.cuda.empty_cache()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.scan_parquet(\"data/preprocessed/articles.parquet\").collect().to_pandas()\n",
    "texts = data.text.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentence mean pooled normalized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e9f74939164b8e8c115e1a8469bcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding batches:   0%|          | 0/39607 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0521 09:42:09.662000 712855 torch/_inductor/utils.py:1137] [1/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    }
   ],
   "source": [
    "pooled_embeddings = batch_embed(texts=texts, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/preprocessed/embeddings_mp.npy\", pooled_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: this code is 9x faster than SBERT implementation because of using low-level optimization:\n",
    "\n",
    "1. We don't use multiprocessing so we avoid serialization delays\n",
    "2. Straight using FlashAttention-2 instead of common Transformer attention\n",
    "3. Auto device mapping that allow optimal weights distribution\n",
    "4. Mixed precise (float16) instead of float32, that allow get embeddings much faster without crucial precision losses\n",
    "5. Inference mode turn off gradient calculation decrease PyTorch Autograd overhead and save about 20-30% time\n",
    "6. We don't use any convertations like .to_tensor() or .to_numpy()\n",
    "7. Cache cleaning makes memory more stable and proccess of embeddings extraction faster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
