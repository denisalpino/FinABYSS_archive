{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from cuml.preprocessing import normalize\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import os\n",
    "os.chdir('/home/denisalpino/dev/FinABYSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda device is available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"{} device is available\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure embedding model based on fine-tuned ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"Alibaba-NLP/gte-modernbert-base\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Alibaba-NLP/gte-modernbert-base\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_embed(texts, batch_size=32):\n",
    "    embeddings = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n",
    "            inputs = tokenizer(\n",
    "                texts[i:i+batch_size],\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "                max_length=8192,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\"):\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Mean pooling\n",
    "            attn_mask = inputs.attention_mask.unsqueeze(-1)\n",
    "            pooled = torch.sum(outputs.last_hidden_state * attn_mask, dim=1) / torch.clamp(attn_mask.sum(dim=1), min=1e-9)\n",
    "            # Mixed Precision\n",
    "            embeddings.append(pooled.half().cpu())\n",
    "            # Cache cleaning\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    result = torch.cat(embeddings).numpy().reshape(len(texts), 768)\n",
    "    torch.cuda.empty_cache()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.scan_parquet(\"data/preprocessed/articles.parquet\").collect().to_pandas()\n",
    "texts = data.text.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentence mean pooled normalized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_embeddings = batch_embed(texts=texts, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_pooled_embeddings = normalize(pooled_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/preprocessed/embeddings_mp_norm.npy\", normalized_pooled_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: this code is 9x faster than SBERT implementation because of using low-level optimization:\n",
    "\n",
    "1. We don't use multiprocessing so we avoid serialization delays\n",
    "2. Straight using FlashAttention-2 instead of common Transformer attention\n",
    "3. Auto device mapping that allow optimal weights distribution\n",
    "4. Mixed precise (float16) instead of float32, that allow get embeddings much faster without crucial precision losses\n",
    "5. Inference mode turn off gradient calculation decrease PyTorch Autograd overhead and save about 20-30% time\n",
    "6. We don't use any convertations like .to_tensor() or .to_numpy()\n",
    "7. Cache cleaning makes memory more stable and proccess of embeddings extraction faster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
